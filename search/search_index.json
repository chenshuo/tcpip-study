{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Shuo Chen's notes on Linux TCP/IP stack Source of this site: https://github.com/chenshuo/tcpip-study The TCP state machine has three inputs: Sockets API, Timers, and Data arrives. Overview of packet flow: A closer look at Linux networking datapath callgraph: TCP/IP Reference TCP/IP Illustrated (vol. 1): The Protocols, 2nd ed. by Kevin R. Fall and W. Richard Stevens , 2011-11. RFC793 Transmission Control Protocol, 1981-09 RFC896 Nagle algorithm, 1984-01. Minshall's update . RFC1122 Requirements for Internet Hosts --- Communication Layers, 1989-10 RFC5681 TCP Congestion Control, 2009-09 RFC6093 On the Implementation of the TCP Urgent Mechanism, 2011-01, which recommends against the use of urgent mechanism. RFC6582 The NewReno Modification to TCP's Fast Recovery Algorithm, 2012-04 RFC7323 TCP Extensions for High Performance, obsoletes RFC1323 RFC7413 TCP Fast Open RFC7414 TCP Roadmap RFC8289 Controlled Delay Active Queue Management RFC8312 CC- CUBIC , Linux's default congestion control algorithm since 2.6.19, replaced BIC (default from 2.6.8 till 2.6.18.x). FreeBSD will use CUBIC as the new default , replacing NewReno . RFC8985 RACK-TLP Loss Detection Algorithm for TCP RFC9293 Transmission Control Protocol (TCP), obsoletes RFC793 after 40+ yrs, 2022-08. Many others in 'Links' page. TCP/IP Implementations AFAIK, there are four independent mainstream TCP/IP stacks: BSD, Linux, Windows, and Solaris ( Mentat TCP and archived page . I guess BSD stack is also used on macOS and iOS, Android uses Linux stack. So I guess most of traffic on Internet happens between the first three TCP/IP stacks. BSD family, BSD family tree 4.2BSD was the first widely available TCP/IP implementation. 4.4BSD-Lite is convered in great detail in TCP/IP Illustrated (vol. 2): The Implementation by Gary R. Wright and W. Richard Stevens, 1995. FreeBSD , http://caia.swin.edu.au/urp/newtcp/ http://www.f-stack.org/ User space TCP/IP stack from FreeBSD 11.0, https://github.com/pkelsey/libuinet . Linux, some early history First in 0.98 by Ross Biro, net/tcp , 1992-09-29 Switched to a new one (NET2) by Fred van Kempen in 0.99.10, net/inet , 1993-06-07 NET3 by Swansea University Computer Society (Alan Cox) took place in 1.1.4. In 1.2.10 -> 1.3.0, moved from net/inet to net/ipv4 . Last update to net/inet was in 1.2.13 In 2.1.8, added net/ipv6 In 2.2.0pre5, renamed to NET4, early 1999. https://blog.cloudflare.com/why-we-use-the-linux-kernels-tcp-stack/ https://jvns.ca/blog/2016/06/30/why-do-we-use-the-linux-kernels-tcp-stack/ lwIP / uIP / picoTCP For microcontrollers, small footprint gvisor / netstack User space, in Golang Others, mostly user space https://shader.kaist.edu/mtcp/ http://seastar.io/networking/ Educational OSes Minix 2.x has its own TCP/IP stack, 3.x uses lwIP instead. Xinu code , covered in Internetworking With TCP/IP Volume II: Design, Implementation, and Internals, 3rd ed. by Douglas E. Comer and David L. Stevens, 1999. Toy implementations https://github.com/saminiir/level-ip https://github.com/chobits/tapip Stanford CS144 https://cs144.github.io Tools packetdrill is a unittest for entire TCP/IP stack. neper is a performance testing tool to generate workloads. Recent changes Recent changes that I am aware of. EDT netdev 0x12 Keynote : Evolving from AFAP: Teaching NICs about time by Van Jacobson , slides and video . Linux 4.20 switched to Early Departure Time model in 2018/09, and refined in 2018/10. Historical notes In 2004, Vinton Cerf and Robert Kahn received the ACM Turing Award for their foundational work on TCP/IP. A Protocol for Packet Network Intercommunication , May 1974.","title":"Home"},{"location":"#shuo-chens-notes-on-linux-tcpip-stack","text":"Source of this site: https://github.com/chenshuo/tcpip-study The TCP state machine has three inputs: Sockets API, Timers, and Data arrives. Overview of packet flow: A closer look at Linux networking datapath callgraph:","title":"Shuo Chen's notes on Linux TCP/IP stack"},{"location":"#tcpip-reference","text":"TCP/IP Illustrated (vol. 1): The Protocols, 2nd ed. by Kevin R. Fall and W. Richard Stevens , 2011-11. RFC793 Transmission Control Protocol, 1981-09 RFC896 Nagle algorithm, 1984-01. Minshall's update . RFC1122 Requirements for Internet Hosts --- Communication Layers, 1989-10 RFC5681 TCP Congestion Control, 2009-09 RFC6093 On the Implementation of the TCP Urgent Mechanism, 2011-01, which recommends against the use of urgent mechanism. RFC6582 The NewReno Modification to TCP's Fast Recovery Algorithm, 2012-04 RFC7323 TCP Extensions for High Performance, obsoletes RFC1323 RFC7413 TCP Fast Open RFC7414 TCP Roadmap RFC8289 Controlled Delay Active Queue Management RFC8312 CC- CUBIC , Linux's default congestion control algorithm since 2.6.19, replaced BIC (default from 2.6.8 till 2.6.18.x). FreeBSD will use CUBIC as the new default , replacing NewReno . RFC8985 RACK-TLP Loss Detection Algorithm for TCP RFC9293 Transmission Control Protocol (TCP), obsoletes RFC793 after 40+ yrs, 2022-08. Many others in 'Links' page.","title":"TCP/IP Reference"},{"location":"#tcpip-implementations","text":"AFAIK, there are four independent mainstream TCP/IP stacks: BSD, Linux, Windows, and Solaris ( Mentat TCP and archived page . I guess BSD stack is also used on macOS and iOS, Android uses Linux stack. So I guess most of traffic on Internet happens between the first three TCP/IP stacks. BSD family, BSD family tree 4.2BSD was the first widely available TCP/IP implementation. 4.4BSD-Lite is convered in great detail in TCP/IP Illustrated (vol. 2): The Implementation by Gary R. Wright and W. Richard Stevens, 1995. FreeBSD , http://caia.swin.edu.au/urp/newtcp/ http://www.f-stack.org/ User space TCP/IP stack from FreeBSD 11.0, https://github.com/pkelsey/libuinet . Linux, some early history First in 0.98 by Ross Biro, net/tcp , 1992-09-29 Switched to a new one (NET2) by Fred van Kempen in 0.99.10, net/inet , 1993-06-07 NET3 by Swansea University Computer Society (Alan Cox) took place in 1.1.4. In 1.2.10 -> 1.3.0, moved from net/inet to net/ipv4 . Last update to net/inet was in 1.2.13 In 2.1.8, added net/ipv6 In 2.2.0pre5, renamed to NET4, early 1999. https://blog.cloudflare.com/why-we-use-the-linux-kernels-tcp-stack/ https://jvns.ca/blog/2016/06/30/why-do-we-use-the-linux-kernels-tcp-stack/ lwIP / uIP / picoTCP For microcontrollers, small footprint gvisor / netstack User space, in Golang Others, mostly user space https://shader.kaist.edu/mtcp/ http://seastar.io/networking/ Educational OSes Minix 2.x has its own TCP/IP stack, 3.x uses lwIP instead. Xinu code , covered in Internetworking With TCP/IP Volume II: Design, Implementation, and Internals, 3rd ed. by Douglas E. Comer and David L. Stevens, 1999. Toy implementations https://github.com/saminiir/level-ip https://github.com/chobits/tapip Stanford CS144 https://cs144.github.io","title":"TCP/IP Implementations"},{"location":"#tools","text":"packetdrill is a unittest for entire TCP/IP stack. neper is a performance testing tool to generate workloads.","title":"Tools"},{"location":"#recent-changes","text":"Recent changes that I am aware of. EDT netdev 0x12 Keynote : Evolving from AFAP: Teaching NICs about time by Van Jacobson , slides and video . Linux 4.20 switched to Early Departure Time model in 2018/09, and refined in 2018/10.","title":"Recent changes"},{"location":"#historical-notes","text":"In 2004, Vinton Cerf and Robert Kahn received the ACM Turing Award for their foundational work on TCP/IP. A Protocol for Packet Network Intercommunication , May 1974.","title":"Historical notes"},{"location":"links/","text":"Links Literatures I have or haven't read. Optimizing TCP for high WAN throughput while preserving low latency Tuning Linux TCP for data-center networks by Yuchung Cheng, Linux Plumbers 2022. Socksdirect: datacenter sockets can be fast and compatible , SIGCOMM '19. PDF Busypolling next generation by Eric Dumazet, 2017. Making Linux TCP Fast by Yuchung Cheng and Neal Cardwell, 2016. paper Kernel Networking Walkthrough by Thomas Graf (tglx), LinuxCon 2015. Nice and short (~20 slides) intro to NAPI, RSS, RPS, GRO, TSO, FastOpen with pictures. Next year: Kernel Networking Explained also by Thomas Graf, LinuxCon 2016, 27 slides. Linux Networking Architecture slides by Hugo Lu, 2014. Queueing in the Linux Network Stack , 2013. TCP small queues , LWN 2012. Controlling Queue Delay by Kathleen Nichols and Van Jacobson, ACM Queue May 2012. The classification and evolution of variants of TCP congestion control EECS 489: Computer Networks at umich.edu You don't know jack about Network Performance by Kevin Fall and Steve McCanne, ACM Queue June 2005. Clearly explains four types of network delay . TCP Implementation in Linux: A Brief Tutorial , 2008. Nice two-page overview of TCP/IP stack in Linux 2.6.19. Scaling in the Linux Networking Stack , kernel doc that describes RSS, RPS, RFS, XPS, etc. Segmentation Offloads in the Linux Networking Stack , about TSO, GSO, GRO, etc. Programming with the Netpoll API by Jeff Moyer, Linux Kongress 2005. Kernel data flow of 2.6.20 RFCs RFC1958 Architectural Principles of the Internet, 1996-06. RFC2525 Known TCP Implementation Problems, 1999-03. RFC2544 Benchmarking Methodology for Network Interconnect Devices, 1999-03. RFC3150 End-to-end Performance Implications of Slow Links, 2001-07. RFC3439 Some Internet Architectural Guidelines and Philosophy, 2002-12. \"Layering Considered Harmful.\" linked from Internet protocol suite RFC6349 Framework for TCP Throughput Testing, 2011-08. RFC8900 IP Fragmentation Considered Fragile RFC9006 TCP Usage Guidance in the Internet of Things (IoT), 2021-03. RFC drafts https://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01 , early version of RFC8985 ? https://tools.ietf.org/html/draft-cardwell-iccrg-bbr-congestion-control-00 Posts Segmentation and Checksum Offloading: Turning Off with ethtool by Dr Steven Gordon, 2010 Reply from David Miller about capturing packets when GSO is on. https://calomel.org/network_loss_emulation.html https://spl0dge.wordpress.com/2013/09/08/building-a-wan-simulator/ https://web.archive.org/web/20160306040049/http://tdistler.com/2011/06/10/netem-wan-emulation-how-to-setup-a-netem-box set up a bridge and introduce packet loss and delay. John Nagle on Nagle's algorithm https://news.ycombinator.com/item?id=9048947 , 2015. Sigh. If you're doing bulk file transfers, you never hit that problem. If you're sending enough data to fill up outgoing buffers, there's no delay. If you send all the data and close the TCP connection, there's no delay after the last packet. If you do send, reply, send, reply, there's no delay. If you do bulk sends, there's no delay. If you do send, send, reply, there's a delay. Coping with the TCP TIME-WAIT state on busy Linux servers Harping on ARP , Multiple Interfaces on Same Ethernet Broadcast Network Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window","title":"Links"},{"location":"links/#links","text":"Literatures I have or haven't read. Optimizing TCP for high WAN throughput while preserving low latency Tuning Linux TCP for data-center networks by Yuchung Cheng, Linux Plumbers 2022. Socksdirect: datacenter sockets can be fast and compatible , SIGCOMM '19. PDF Busypolling next generation by Eric Dumazet, 2017. Making Linux TCP Fast by Yuchung Cheng and Neal Cardwell, 2016. paper Kernel Networking Walkthrough by Thomas Graf (tglx), LinuxCon 2015. Nice and short (~20 slides) intro to NAPI, RSS, RPS, GRO, TSO, FastOpen with pictures. Next year: Kernel Networking Explained also by Thomas Graf, LinuxCon 2016, 27 slides. Linux Networking Architecture slides by Hugo Lu, 2014. Queueing in the Linux Network Stack , 2013. TCP small queues , LWN 2012. Controlling Queue Delay by Kathleen Nichols and Van Jacobson, ACM Queue May 2012. The classification and evolution of variants of TCP congestion control EECS 489: Computer Networks at umich.edu You don't know jack about Network Performance by Kevin Fall and Steve McCanne, ACM Queue June 2005. Clearly explains four types of network delay . TCP Implementation in Linux: A Brief Tutorial , 2008. Nice two-page overview of TCP/IP stack in Linux 2.6.19. Scaling in the Linux Networking Stack , kernel doc that describes RSS, RPS, RFS, XPS, etc. Segmentation Offloads in the Linux Networking Stack , about TSO, GSO, GRO, etc. Programming with the Netpoll API by Jeff Moyer, Linux Kongress 2005. Kernel data flow of 2.6.20","title":"Links"},{"location":"links/#rfcs","text":"RFC1958 Architectural Principles of the Internet, 1996-06. RFC2525 Known TCP Implementation Problems, 1999-03. RFC2544 Benchmarking Methodology for Network Interconnect Devices, 1999-03. RFC3150 End-to-end Performance Implications of Slow Links, 2001-07. RFC3439 Some Internet Architectural Guidelines and Philosophy, 2002-12. \"Layering Considered Harmful.\" linked from Internet protocol suite RFC6349 Framework for TCP Throughput Testing, 2011-08. RFC8900 IP Fragmentation Considered Fragile RFC9006 TCP Usage Guidance in the Internet of Things (IoT), 2021-03.","title":"RFCs"},{"location":"links/#rfc-drafts","text":"https://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01 , early version of RFC8985 ? https://tools.ietf.org/html/draft-cardwell-iccrg-bbr-congestion-control-00","title":"RFC drafts"},{"location":"links/#posts","text":"Segmentation and Checksum Offloading: Turning Off with ethtool by Dr Steven Gordon, 2010 Reply from David Miller about capturing packets when GSO is on. https://calomel.org/network_loss_emulation.html https://spl0dge.wordpress.com/2013/09/08/building-a-wan-simulator/ https://web.archive.org/web/20160306040049/http://tdistler.com/2011/06/10/netem-wan-emulation-how-to-setup-a-netem-box set up a bridge and introduce packet loss and delay. John Nagle on Nagle's algorithm https://news.ycombinator.com/item?id=9048947 , 2015. Sigh. If you're doing bulk file transfers, you never hit that problem. If you're sending enough data to fill up outgoing buffers, there's no delay. If you send all the data and close the TCP connection, there's no delay after the last packet. If you do send, reply, send, reply, there's no delay. If you do bulk sends, there's no delay. If you do send, send, reply, there's a delay. Coping with the TCP TIME-WAIT state on busy Linux servers Harping on ARP , Multiple Interfaces on Same Ethernet Broadcast Network Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window","title":"Posts"},{"location":"profile/","text":"Profiling Linux TCP/IP stack with perf and pprof At home, I have two Linux hosts with Mellanox 10GbE nic (ConnectX EN 10GigE MT26448, bought used from Ebay in 2017) directly connected using SPF cable. Thoughput was about 1100MiB/s over 10GbE, both machine runs ~40% CPU utilization in one thread. For comparison, run openssl speed sha on the Rx side machine, an i7-3770 @ 3.4GHz. $ openssl speed sha OpenSSL 1.1.1f 31 Mar 2020 The 'numbers' are in 1000s of bytes per second processed. type 16 bytes 64 bytes 256 bytes 1024 bytes 8192 bytes 16384 bytes sha1 137355.08k 323943.17k 603290.54k 770941.95k 843352.75k 849619.63k sha256 75659.30k 167113.02k 289437.70k 354737.49k 379652.78k 381676.20k sha512 51745.33k 206941.63k 329443.07k 468301.82k 533897.22k 539525.12k In short, sending data through TCP is faster than calculating SHA1 locally. Rx is more expensive, it uses about 2x CPU cycles than Tx. Tx path Profile taken on Debian bullseye (testing, pre-release 11) w/ kernel 5.6.14. Run the chargen program to keep sending data to a discard server. Rx path Profile taken on Ubuntu 18.04 w/ kernel 4.15 Run the discard program to keep reading the socket. Loopback w/ IPv6 Profile taken on Ubuntu 20.04 w/ kernel 5.4. Run both chargen and discard on the same i7-3770 host, throughput was about 3300MiB/s. chargen ran at 100% CPU, discard was about 74%.","title":"Profiling"},{"location":"profile/#profiling-linux-tcpip-stack-with-perf-and-pprof","text":"At home, I have two Linux hosts with Mellanox 10GbE nic (ConnectX EN 10GigE MT26448, bought used from Ebay in 2017) directly connected using SPF cable. Thoughput was about 1100MiB/s over 10GbE, both machine runs ~40% CPU utilization in one thread. For comparison, run openssl speed sha on the Rx side machine, an i7-3770 @ 3.4GHz. $ openssl speed sha OpenSSL 1.1.1f 31 Mar 2020 The 'numbers' are in 1000s of bytes per second processed. type 16 bytes 64 bytes 256 bytes 1024 bytes 8192 bytes 16384 bytes sha1 137355.08k 323943.17k 603290.54k 770941.95k 843352.75k 849619.63k sha256 75659.30k 167113.02k 289437.70k 354737.49k 379652.78k 381676.20k sha512 51745.33k 206941.63k 329443.07k 468301.82k 533897.22k 539525.12k In short, sending data through TCP is faster than calculating SHA1 locally. Rx is more expensive, it uses about 2x CPU cycles than Tx.","title":"Profiling Linux TCP/IP stack with perf and pprof"},{"location":"profile/#tx-path","text":"Profile taken on Debian bullseye (testing, pre-release 11) w/ kernel 5.6.14. Run the chargen program to keep sending data to a discard server.","title":"Tx path"},{"location":"profile/#rx-path","text":"Profile taken on Ubuntu 18.04 w/ kernel 4.15 Run the discard program to keep reading the socket.","title":"Rx path"},{"location":"profile/#loopback-w-ipv6","text":"Profile taken on Ubuntu 20.04 w/ kernel 5.4. Run both chargen and discard on the same i7-3770 host, throughput was about 3300MiB/s. chargen ran at 100% CPU, discard was about 74%.","title":"Loopback w/ IPv6"},{"location":"reno/","text":"Reno Congestion Control Classic Reno CC algorithm fits in one page from linux/net/ipv4/tcp_cong.c /* * TCP Reno congestion control * This is special case used for fallback as well. */ /* This is Jacobson's slow start and congestion avoidance. * SIGCOMM '88, p. 328. */ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked) { struct tcp_sock *tp = tcp_sk(sk); if (!tcp_is_cwnd_limited(sk)) return; /* In \"safe\" area, increase. */ if (tcp_in_slow_start(tp)) { acked = tcp_slow_start(tp, acked); if (!acked) return; } /* In dangerous area, increase slowly. */ tcp_cong_avoid_ai(tp, tcp_snd_cwnd(tp), acked); } EXPORT_SYMBOL_GPL(tcp_reno_cong_avoid); /* Slow start is used when congestion window is no greater than the slow start * threshold. We base on RFC2581 and also handle stretch ACKs properly. * We do not implement RFC3465 Appropriate Byte Counting (ABC) per se but * something better;) a packet is only considered (s)acked in its entirety to * defend the ACK attacks described in the RFC. Slow start processes a stretch * ACK of degree N as if N acks of degree 1 are received back to back except * ABC caps N to 2. Slow start exits when cwnd grows over ssthresh and * returns the leftover acks to adjust cwnd in congestion avoidance mode. */ u32 tcp_slow_start(struct tcp_sock *tp, u32 acked) { u32 cwnd = min(tcp_snd_cwnd(tp) + acked, tp->snd_ssthresh); acked -= cwnd - tcp_snd_cwnd(tp); tcp_snd_cwnd_set(tp, min(cwnd, tp->snd_cwnd_clamp)); return acked; } EXPORT_SYMBOL_GPL(tcp_slow_start); /* In theory this is tp->snd_cwnd += 1 / tp->snd_cwnd (or alternative w), * for every packet that was ACKed. */ void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked) { // I guess \"ai\" here stands for additive increase // ... } /* Slow start threshold is half the congestion window (min 2) */ u32 tcp_reno_ssthresh(struct sock *sk) { const struct tcp_sock *tp = tcp_sk(sk); return max(tcp_snd_cwnd(tp) >> 1U, 2U); } EXPORT_SYMBOL_GPL(tcp_reno_ssthresh); Where tcp_in_slow_start() and tcp_is_cwnd_limited() are defined in `linux/include/net/tcp.h': static inline bool tcp_in_slow_start(const struct tcp_sock *tp) { return tcp_snd_cwnd(tp) < tp->snd_ssthresh; } /* We follow the spirit of RFC2861 to validate cwnd but implement a more * flexible approach. The RFC suggests cwnd should not be raised unless * it was fully used previously. And that's exactly what we do in * congestion avoidance mode. But in slow start we allow cwnd to grow * as long as the application has used half the cwnd. * Example : * cwnd is 10 (IW10), but application sends 9 frames. * We allow cwnd to reach 18 when all frames are ACKed. * This check is safe because it's as aggressive as slow start which already * risks 100% overshoot. The advantage is that we discourage application to * either send more filler packets or data to artificially blow up the cwnd * usage, and allow application-limited process to probe bw more aggressively. */ static inline bool tcp_is_cwnd_limited(const struct sock *sk) { const struct tcp_sock *tp = tcp_sk(sk); if (tp->is_cwnd_limited) return true; /* If in slow start, ensure cwnd grows to twice what was ACKed. */ if (tcp_in_slow_start(tp)) return tcp_snd_cwnd(tp) < 2 * tp->max_packets_out; return false; } tcp_reno_cong_avoid() gets called from tcp_rcv_established() . tcp_rcv_established() -> tcp_queue_rcv() // If income segment have payload -> tcp_ack() // This routine deals with incoming acks, but not outgoing ones. -> tcp_cong_control() // Common entry point, called toward the end of // processing an ACK with precise rate info. // All transmission or retransmission are delayed afterwards. -> if tcp_in_cwnd_reduction(): tcp_cwnd_reduction() elif tcp_may_raise_cwnd(): tcp_cong_avoid(): icsk->icsk_ca_ops->cong_avoid() -> tcp_reno_cong_avoid() or cubictcp_cong_avoid() -> tcp_data_snd_check() // If we can send more data -> tcp_push_pending_frames() -> tcp_check_space() // check if new space made available -> tcp_new_space() // if SOCK_NOSPACE is set in sk->sk_socket->flags -> if tcp_should_expand_sndbuf(): tcp_sndbuf_expand() -> __tcp_ack_snd_check() // Check if sending an ack is needed.","title":"Reno CC"},{"location":"reno/#reno-congestion-control","text":"Classic Reno CC algorithm fits in one page from linux/net/ipv4/tcp_cong.c /* * TCP Reno congestion control * This is special case used for fallback as well. */ /* This is Jacobson's slow start and congestion avoidance. * SIGCOMM '88, p. 328. */ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked) { struct tcp_sock *tp = tcp_sk(sk); if (!tcp_is_cwnd_limited(sk)) return; /* In \"safe\" area, increase. */ if (tcp_in_slow_start(tp)) { acked = tcp_slow_start(tp, acked); if (!acked) return; } /* In dangerous area, increase slowly. */ tcp_cong_avoid_ai(tp, tcp_snd_cwnd(tp), acked); } EXPORT_SYMBOL_GPL(tcp_reno_cong_avoid); /* Slow start is used when congestion window is no greater than the slow start * threshold. We base on RFC2581 and also handle stretch ACKs properly. * We do not implement RFC3465 Appropriate Byte Counting (ABC) per se but * something better;) a packet is only considered (s)acked in its entirety to * defend the ACK attacks described in the RFC. Slow start processes a stretch * ACK of degree N as if N acks of degree 1 are received back to back except * ABC caps N to 2. Slow start exits when cwnd grows over ssthresh and * returns the leftover acks to adjust cwnd in congestion avoidance mode. */ u32 tcp_slow_start(struct tcp_sock *tp, u32 acked) { u32 cwnd = min(tcp_snd_cwnd(tp) + acked, tp->snd_ssthresh); acked -= cwnd - tcp_snd_cwnd(tp); tcp_snd_cwnd_set(tp, min(cwnd, tp->snd_cwnd_clamp)); return acked; } EXPORT_SYMBOL_GPL(tcp_slow_start); /* In theory this is tp->snd_cwnd += 1 / tp->snd_cwnd (or alternative w), * for every packet that was ACKed. */ void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked) { // I guess \"ai\" here stands for additive increase // ... } /* Slow start threshold is half the congestion window (min 2) */ u32 tcp_reno_ssthresh(struct sock *sk) { const struct tcp_sock *tp = tcp_sk(sk); return max(tcp_snd_cwnd(tp) >> 1U, 2U); } EXPORT_SYMBOL_GPL(tcp_reno_ssthresh); Where tcp_in_slow_start() and tcp_is_cwnd_limited() are defined in `linux/include/net/tcp.h': static inline bool tcp_in_slow_start(const struct tcp_sock *tp) { return tcp_snd_cwnd(tp) < tp->snd_ssthresh; } /* We follow the spirit of RFC2861 to validate cwnd but implement a more * flexible approach. The RFC suggests cwnd should not be raised unless * it was fully used previously. And that's exactly what we do in * congestion avoidance mode. But in slow start we allow cwnd to grow * as long as the application has used half the cwnd. * Example : * cwnd is 10 (IW10), but application sends 9 frames. * We allow cwnd to reach 18 when all frames are ACKed. * This check is safe because it's as aggressive as slow start which already * risks 100% overshoot. The advantage is that we discourage application to * either send more filler packets or data to artificially blow up the cwnd * usage, and allow application-limited process to probe bw more aggressively. */ static inline bool tcp_is_cwnd_limited(const struct sock *sk) { const struct tcp_sock *tp = tcp_sk(sk); if (tp->is_cwnd_limited) return true; /* If in slow start, ensure cwnd grows to twice what was ACKed. */ if (tcp_in_slow_start(tp)) return tcp_snd_cwnd(tp) < 2 * tp->max_packets_out; return false; } tcp_reno_cong_avoid() gets called from tcp_rcv_established() . tcp_rcv_established() -> tcp_queue_rcv() // If income segment have payload -> tcp_ack() // This routine deals with incoming acks, but not outgoing ones. -> tcp_cong_control() // Common entry point, called toward the end of // processing an ACK with precise rate info. // All transmission or retransmission are delayed afterwards. -> if tcp_in_cwnd_reduction(): tcp_cwnd_reduction() elif tcp_may_raise_cwnd(): tcp_cong_avoid(): icsk->icsk_ca_ops->cong_avoid() -> tcp_reno_cong_avoid() or cubictcp_cong_avoid() -> tcp_data_snd_check() // If we can send more data -> tcp_push_pending_frames() -> tcp_check_space() // check if new space made available -> tcp_new_space() // if SOCK_NOSPACE is set in sk->sk_socket->flags -> if tcp_should_expand_sndbuf(): tcp_sndbuf_expand() -> __tcp_ack_snd_check() // Check if sending an ack is needed.","title":"Reno Congestion Control"},{"location":"slowstart/","text":"TCP Slow Start Standard slow start RFC5681 : Cwnd increases MSS upon receipt of an ACK covering new data of MSS. But Linux and FreeBSD differ when bytes_acked > 2 * MSS . Effectively Cwnd doubles every round-trip time, Cwnd = IW * 2 ^ nRTT. Exits when a packet loss is detected, sets ssthresh to Cwnd/2, as per Reno CC. Linux and FreeBSD don't increase Cwnd if transmitting is not limited by Cwnd, see cubictcp_cong_avoid() in tcp_cubic.c and tcp_reno_cong_avoid() in tcp_cong.c ). With Initial Window = 10: Kernel knobs $ sysctl -A |grep tcp_.mem # min default max net.ipv4.tcp_rmem = 4096 131072 6291456 net.ipv4.tcp_wmem = 4096 16384 4194304 For tcp_wmem[1] == 16K , the default sndbuf is 16K. Which is greater than 10 (initial window) * 1460 (typical IPv4 MSS), works well for slow start. Linux 4.20 changed tcp_rmem[1] from 87k to 128KiB, commit by Yuchung Cheng in 2018-09 . So SYN rwin increased from 43k to 65k (when tcp_adv_win_scale == 1 ). With IW=10 and MSS=1.4k, during slow start, the old setting will hit window full on 3rd RTT, limit sent bytes to ~70kB. In new setting, 1.4 * (10+20+40) ~= 100k can be sent in 3RTT. From linux-stable/Documentation/networking/ip-sysctl.rst : tcp_adv_win_scale - INTEGER Count buffering overhead as bytes/2^tcp_adv_win_scale (if tcp_adv_win_scale > 0) or bytes-bytes/2^(-tcp_adv_win_scale), if it is <= 0. Default: 1 In other words, tcp_adv_win_scale Advertised window ratio Max adv win when tcp_rmem[2] == 8M 0 100% 8M 1 (default since Linux 3.4) 50% 4M 2 75% 6M 3 87.5% 7M -1 50% 4M -2 25% 2M -3 12.5% 1M This value was changed in Linux 3.4 from 2 to 1. Here's a brief history: Kernel Version tcp_adv_win_scale sysctl tcp_rmem[] Initial advertised rcvwnd Max rcvwnd commit Before 3.4 2 \"4096 87380 4MiB\" 65535 = (87380 * 0.75) 3MiB = (4MiB * 0.75) 3.4 to 4.19 1 \"4096 87380 6MiB\" 43800 = (87380 * 0.5) 3MiB = (6MiB * 0.5) 2012-05 Since 4.20 1 \"4096 128KiB 6MiB\" 64Ki = (128Ki * 0.5) 3MiB = (6MiB * 0.5) 2018-09 HyStart++ Stardard slow start ends when a packet loss is detected, but this often causes overshoot. HyStart++ uses \"increase in round-trip delay\" as a heuristic to find an exit point before possible overshoot. RFC9406 HyStart++: Modified Slow Start for TCP, 2023-05. Linux incorporated HyStart++ to CUBIC in v2.6.29, 2009. commit by Sangtae Ha . FreeBSD adds HyStart++ to its newreno CC https://reviews.freebsd.org/D32373 in 2021, but not released as of 13.2. FreeBSD will switch to CUBIC (in release 14?). https://blog.cloudflare.com/cubic-and-hystart-support-in-quiche/ FreeBSD As noticed in Low throughput due to small cwnd , FreeBSD slow-start is sometimes much slower than Linux, and underutilizes the bandwidth of a link with long delay (say 50ms ~ 100ms). As I analyzed in https://lists.freebsd.org/archives/freebsd-net/2023-May/003282.html , it's due to bad interaction with LRO and delayed-ACKs of receiver side. RFC 5681 states that During slow start, a TCP increments cwnd by at most SMSS bytes for each ACK received that cumulatively acknowledges new data. While traditionally TCP implementations have increased cwnd by precisely SMSS bytes upon receipt of an ACK covering new data, we RECOMMEND that TCP implementations increase cwnd, per: cwnd += min (N, SMSS) where N is the number of previously unacknowledged bytes acknowledged in the incoming ACK. If one ACK is generated per SMSS, the cwnd grows exponentially. In old times, RTT1: cwnd = 1, send 1 MSS RTT2: got 1st ACK, cwnd = 2, send 2 MSS RTT3: got 2nd ACK, cwnd = 3, send 2 MSS got 3rd ACK, cwnd = 4, send 2 MSS RTT4: got 4 ACKs, cwnd = 8, send 8 MSS RTT5: got 8 ACKs, cwnd = 16, send 16 MSS RFC 5681 also requires that A receiver SHOULD generate an ACK for at least every second full-sized segment. If receiver follows this, the slow-start should work just fine. But in case of LRO and TSO, the sender sees much less ACKs than the old days. Really slow start: RTT1: cwnd = 10, send segment of 10 MSS with TSO RTT2: got 1st ACK (LRO in receiver), cwnd = 12, send 12 MSS with TSO RTT3: got 2nd ACK, cwnd = 14, send 14 MSS with TSO RTT4: got 3rd ACK, cwnd = 16, send 16 MSS with TSO RTT5: got 4th ACK, cwnd = 18, send 18 MSS with TSO So cwnd grows more-or-less linearly until it reaches max TSO segments (65k / 1.4k =~ 45). I intuitively guess cwnd grows quadratically after that, i.e. two segments/ACKs per RTT, then three segments/ACKs per RTT, and so on. FreeBSD 13.x TCP sender closely follows RFC 5681 with RFC 3465 extension, It also addressed the LRO of the sender side (multiple ACKs being aggregated into one). sys/netinet/cc/cc_newreno.c static void newreno_ack_received(struct cc_var *ccv, uint16_t type) { // ... /* * Regular in-order ACK, open the congestion window. * Method depends on which congestion control state we're * in (slow start or cong avoid) and if ABC (RFC 3465) is * enabled. * * slow start: cwnd <= ssthresh * cong avoid: cwnd > ssthresh * * slow start and ABC (RFC 3465): * Grow cwnd exponentially by the amount of data * ACKed capping the max increment per ACK to * (abc_l_var * maxseg) bytes. * * slow start without ABC (RFC 5681): * Grow cwnd exponentially by maxseg per ACK. * * ... */ // In slow-start if (V_tcp_do_rfc3465) { uint16_t abc_val; if (ccv->flags & CCF_USE_LOCAL_ABC) abc_val = ccv->labc; else abc_val = V_tcp_abc_l_var; // sysctl value, default = 2 // abc_val is 1 by default // ccv->nsegs is number of ACKs being aggregated due to LRO if (CCV(ccv, snd_nxt) == CCV(ccv, snd_max)) incr = min(ccv->bytes_this_ack, ccv->nsegs * abc_val * CCV(ccv, t_maxseg)); else incr = min(ccv->bytes_this_ack, CCV(ccv, t_maxseg)); } // incr == 2*1448 = 2896 in normal start start case As discussed in sec3.2 of RFC 3465, L=2*SMSS bytes exactly balances the negative impact of the delayed ACK algorithm. But if the receiver (sink) also does LRO, it won't generate enough ACKs to open cwnd of the sender. Often we observe that FreeBSD is slower when sending data using TCP, comparing to Linux or even Windows. https://lists.freebsd.org/archives/freebsd-hackers/2023-April/002082.html As suggested in https://calomel.org/freebsd_network_tuning.html , a large abc_l_var should help in this situation. # TCP Slow start gradually increases the data send rate until the TCP # congestion algorithm (CDG, H-TCP) calculates the networks maximum carrying # capacity without dropping packets. TCP Congestion Control with Appropriate # Byte Counting (ABC) allows our server to increase the maximum congestion # window exponentially by the amount of data ACKed, but limits the maximum # increment per ACK to (abc_l_var * maxseg) bytes. An abc_l_var of 44 times a # maxseg of 1460 bytes would allow slow start to increase the congestion window # by more than 64 kilobytes per step; 65535 bytes is the TCP receive buffer # size of most hosts without TCP window scaling. # net.inet.tcp.abc_l_var=44 # (default 2) if net.inet.tcp.mssdflt = 1460 Traces When h_ertt is on, it will momentarily disable TSO whilst marking a packet to use for a new RTT measurement, resulting in more segments being sent, and more ACK received (from Linux), then cwnd could increase faster. Linux enters 'quickack' mode right after connection establishment, disabling delayed-ACKing for a while, to help peer measuring RTT and accelerate slow-start. But FreeBSD won't, it always delays ACKing (IIUC). // linux-stable/include/net/tcp.h /* Maximal number of ACKs sent quickly to accelerate slow-start. */ #define TCP_MAX_QUICKACKS 16U // linux-stable/net/ipv4/tcp_input.c static void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb) { // ... if (!icsk->icsk_ack.ato) { /* The _first_ data packet received, initialize * delayed ACK engine. */ tcp_incr_quickack(sk, TCP_MAX_QUICKACKS); icsk->icsk_ack.ato = TCP_ATO_MIN; FreeBSD 13 sender, Linux receiver Really slow start on a 100ms link: 0.000000 IP freebsd13 > linux: Flags [S], seq 205083268, win 65535, options [mss 1460,nop,wscale 10,sackOK,TS val 495212525 ecr 0], len 0 0.100786 IP linux > freebsd13: Flags [S.], seq 708257395, ack 205083269, win 65160, options [mss 1460,sackOK,TS val 563185696 ecr 495212525,nop,wscale 7], len 0 0.100800 IP freebsd13 > linux: Flags [.], ack 1, win 65, options [nop,nop,TS val 495212626 ecr 563185696], len 0 // cwnd = 10 0.101062 IP freebsd13 > linux: Flags [.], seq 1:14481, ack 1, win 65, len 14480 0.201241 IP linux > freebsd13: Flags [.], ack 14481, win 427, len 0 // cwnd = 12 0.201253 IP freebsd13 > linux: Flags [.], seq 14481:31857, ack 1, win 65, len 17376 0.301621 IP linux > freebsd13: Flags [.], ack 31857, win 411, len 0 // cwnd = 14 0.301630 IP freebsd13 > linux: Flags [.], seq 31857:52129, ack 1, win 65, len 20272 0.402010 IP linux > freebsd13: Flags [.], ack 52129, win 395, len 0 // cwnd = 16 0.402018 IP freebsd13 > linux: Flags [P.], seq 52129:73629, ack 1, win 65, len 21500 0.402026 IP freebsd13 > linux: Flags [.], seq 73629:75077, ack 1, win 65, len 1448 0.502392 IP linux > freebsd13: Flags [.], ack 75077, win 860, len 0 // cwnd = 18 0.502398 IP freebsd13 > linux: Flags [.], seq 75077:101141, ack 1, win 65, len 26064 0.602775 IP linux > freebsd13: Flags [.], ack 101141, win 1267, len 0 // cwnd = 20 0.602783 IP freebsd13 > linux: Flags [.], seq 101141:130101, ack 1, win 65, len 28960 0.703169 IP linux > freebsd13: Flags [.], ack 130101, win 1719, len 0 // cwnd = 22 0.703177 IP freebsd13 > linux: Flags [P.], seq 130101:159297, ack 1, win 65, len 29196 0.703185 IP freebsd13 > linux: Flags [.], seq 159297:160745, ack 1, win 65, len 1448 0.803367 IP linux > freebsd13: Flags [.], ack 160745, win 2198, len 0 // cwnd = 24 0.803375 IP freebsd13 > linux: Flags [.], seq 160745:195497, ack 1, win 65, len 34752 0.903754 IP linux > freebsd13: Flags [.], ack 195497, win 2741, len 0 // cwnd = 26 0.903762 IP freebsd13 > linux: Flags [.], seq 195497:233145, ack 1, win 65, len 37648 FreeBSD 14 sender, Linux receiver Faster than 13, because h_ertt is on by default. 0.000000 IP freebsd14 > linux: Flags [S], seq 3748224575, win 65535, options [mss 1460,nop,wscale 6,sackOK,TS val 790917200 ecr 0], len 0 0.100788 IP linux > freebsd14: Flags [S.], seq 489912744, ack 3748224576, win 65160, options [mss 1460,sackOK,TS val 380822116 ecr 790917200,nop,wscale 7], len 0 0.100851 IP freebsd14 > linux: Flags [.], ack 1, win 1027, options [nop,nop,TS val 790917303 ecr 380822116], len 0 // cwnd = 10 0.114635 IP freebsd14 > linux: Flags [.], seq 1:14481, ack 1, win 1027, len 14480 0.215265 IP linux > freebsd14: Flags [.], ack 14481, win 445, len 0 // cwnd = 12, h_ertt is on, send two segments, 1 MSS + 11 MSS 0.215334 IP freebsd14 > linux: Flags [.], seq 14481:15929, ack 1, win 1027, len 1448 0.215363 IP freebsd14 > linux: Flags [.], seq 15929:31857, ack 1, win 1027, len 15928 // got ACK for 1 MSS, cwnd = 13, send 2 segments 0.316142 IP linux > freebsd14: Flags [.], ack 15929, win 501, len 0 0.316210 IP freebsd14 > linux: Flags [.], seq 31857:33305, ack 1, win 1027, len 1448 0.316240 IP freebsd14 > linux: Flags [.], seq 33305:34753, ack 1, win 1027, len 1448 // got ACK for 11 MSS, cwnd = 15, send 1 segment of 13 MSS 0.316248 IP linux > freebsd14: Flags [.], ack 31857, win 440, len 0 0.316279 IP freebsd14 > linux: Flags [.], seq 34753:53577, ack 1, win 1027, len 18824 // got ACK for 1 MSS, cwnd = 16, send 2 segments 0.416970 IP linux > freebsd14: Flags [.], ack 33305, win 524, len 0 0.417047 IP freebsd14 > linux: Flags [.], seq 53577:55025, ack 1, win 1027, len 1448 0.417077 IP freebsd14 > linux: Flags [.], seq 55025:56473, ack 1, win 1027, len 1448 // got ACK for 1 MSS, cwnd = 17, send 1 segment of 2 MSS 0.417085 IP linux > freebsd14: Flags [.], ack 34753, win 546, len 0 0.417103 IP freebsd14 > linux: Flags [.], seq 56473:59369, ack 1, win 1027, len 2896 // got ACK for 13 MSS, cwnd = 19, send 1 segment of 15 MSS 0.417111 IP linux > freebsd14: Flags [.], ack 53577, win 840, len 0 0.417146 IP freebsd14 > linux: Flags [.], seq 59369:81089, ack 1, win 1027, len 21720 // And so on 0.517759 IP linux > freebsd14: Flags [.], ack 55025, win 863, len 0 0.517833 IP freebsd14 > linux: Flags [.], seq 81089:82537, ack 1, win 1027, len 1448 0.517862 IP freebsd14 > linux: Flags [.], seq 82537:83985, ack 1, win 1027, len 1448 FreeBSD 13 sender with h_ertt on, same as FreeBSD 14 above FreeBSD 14 sender, FreeBSD receiver Not as fast as Linux reciever, because the receiver delays ACKs.","title":"Slow Start"},{"location":"slowstart/#tcp-slow-start","text":"Standard slow start RFC5681 : Cwnd increases MSS upon receipt of an ACK covering new data of MSS. But Linux and FreeBSD differ when bytes_acked > 2 * MSS . Effectively Cwnd doubles every round-trip time, Cwnd = IW * 2 ^ nRTT. Exits when a packet loss is detected, sets ssthresh to Cwnd/2, as per Reno CC. Linux and FreeBSD don't increase Cwnd if transmitting is not limited by Cwnd, see cubictcp_cong_avoid() in tcp_cubic.c and tcp_reno_cong_avoid() in tcp_cong.c ). With Initial Window = 10:","title":"TCP Slow Start"},{"location":"slowstart/#kernel-knobs","text":"$ sysctl -A |grep tcp_.mem # min default max net.ipv4.tcp_rmem = 4096 131072 6291456 net.ipv4.tcp_wmem = 4096 16384 4194304 For tcp_wmem[1] == 16K , the default sndbuf is 16K. Which is greater than 10 (initial window) * 1460 (typical IPv4 MSS), works well for slow start. Linux 4.20 changed tcp_rmem[1] from 87k to 128KiB, commit by Yuchung Cheng in 2018-09 . So SYN rwin increased from 43k to 65k (when tcp_adv_win_scale == 1 ). With IW=10 and MSS=1.4k, during slow start, the old setting will hit window full on 3rd RTT, limit sent bytes to ~70kB. In new setting, 1.4 * (10+20+40) ~= 100k can be sent in 3RTT. From linux-stable/Documentation/networking/ip-sysctl.rst : tcp_adv_win_scale - INTEGER Count buffering overhead as bytes/2^tcp_adv_win_scale (if tcp_adv_win_scale > 0) or bytes-bytes/2^(-tcp_adv_win_scale), if it is <= 0. Default: 1 In other words, tcp_adv_win_scale Advertised window ratio Max adv win when tcp_rmem[2] == 8M 0 100% 8M 1 (default since Linux 3.4) 50% 4M 2 75% 6M 3 87.5% 7M -1 50% 4M -2 25% 2M -3 12.5% 1M This value was changed in Linux 3.4 from 2 to 1. Here's a brief history: Kernel Version tcp_adv_win_scale sysctl tcp_rmem[] Initial advertised rcvwnd Max rcvwnd commit Before 3.4 2 \"4096 87380 4MiB\" 65535 = (87380 * 0.75) 3MiB = (4MiB * 0.75) 3.4 to 4.19 1 \"4096 87380 6MiB\" 43800 = (87380 * 0.5) 3MiB = (6MiB * 0.5) 2012-05 Since 4.20 1 \"4096 128KiB 6MiB\" 64Ki = (128Ki * 0.5) 3MiB = (6MiB * 0.5) 2018-09","title":"Kernel knobs"},{"location":"slowstart/#hystart","text":"Stardard slow start ends when a packet loss is detected, but this often causes overshoot. HyStart++ uses \"increase in round-trip delay\" as a heuristic to find an exit point before possible overshoot. RFC9406 HyStart++: Modified Slow Start for TCP, 2023-05. Linux incorporated HyStart++ to CUBIC in v2.6.29, 2009. commit by Sangtae Ha . FreeBSD adds HyStart++ to its newreno CC https://reviews.freebsd.org/D32373 in 2021, but not released as of 13.2. FreeBSD will switch to CUBIC (in release 14?). https://blog.cloudflare.com/cubic-and-hystart-support-in-quiche/","title":"HyStart++"},{"location":"slowstart/#freebsd","text":"As noticed in Low throughput due to small cwnd , FreeBSD slow-start is sometimes much slower than Linux, and underutilizes the bandwidth of a link with long delay (say 50ms ~ 100ms). As I analyzed in https://lists.freebsd.org/archives/freebsd-net/2023-May/003282.html , it's due to bad interaction with LRO and delayed-ACKs of receiver side. RFC 5681 states that During slow start, a TCP increments cwnd by at most SMSS bytes for each ACK received that cumulatively acknowledges new data. While traditionally TCP implementations have increased cwnd by precisely SMSS bytes upon receipt of an ACK covering new data, we RECOMMEND that TCP implementations increase cwnd, per: cwnd += min (N, SMSS) where N is the number of previously unacknowledged bytes acknowledged in the incoming ACK. If one ACK is generated per SMSS, the cwnd grows exponentially. In old times, RTT1: cwnd = 1, send 1 MSS RTT2: got 1st ACK, cwnd = 2, send 2 MSS RTT3: got 2nd ACK, cwnd = 3, send 2 MSS got 3rd ACK, cwnd = 4, send 2 MSS RTT4: got 4 ACKs, cwnd = 8, send 8 MSS RTT5: got 8 ACKs, cwnd = 16, send 16 MSS RFC 5681 also requires that A receiver SHOULD generate an ACK for at least every second full-sized segment. If receiver follows this, the slow-start should work just fine. But in case of LRO and TSO, the sender sees much less ACKs than the old days. Really slow start: RTT1: cwnd = 10, send segment of 10 MSS with TSO RTT2: got 1st ACK (LRO in receiver), cwnd = 12, send 12 MSS with TSO RTT3: got 2nd ACK, cwnd = 14, send 14 MSS with TSO RTT4: got 3rd ACK, cwnd = 16, send 16 MSS with TSO RTT5: got 4th ACK, cwnd = 18, send 18 MSS with TSO So cwnd grows more-or-less linearly until it reaches max TSO segments (65k / 1.4k =~ 45). I intuitively guess cwnd grows quadratically after that, i.e. two segments/ACKs per RTT, then three segments/ACKs per RTT, and so on. FreeBSD 13.x TCP sender closely follows RFC 5681 with RFC 3465 extension, It also addressed the LRO of the sender side (multiple ACKs being aggregated into one). sys/netinet/cc/cc_newreno.c static void newreno_ack_received(struct cc_var *ccv, uint16_t type) { // ... /* * Regular in-order ACK, open the congestion window. * Method depends on which congestion control state we're * in (slow start or cong avoid) and if ABC (RFC 3465) is * enabled. * * slow start: cwnd <= ssthresh * cong avoid: cwnd > ssthresh * * slow start and ABC (RFC 3465): * Grow cwnd exponentially by the amount of data * ACKed capping the max increment per ACK to * (abc_l_var * maxseg) bytes. * * slow start without ABC (RFC 5681): * Grow cwnd exponentially by maxseg per ACK. * * ... */ // In slow-start if (V_tcp_do_rfc3465) { uint16_t abc_val; if (ccv->flags & CCF_USE_LOCAL_ABC) abc_val = ccv->labc; else abc_val = V_tcp_abc_l_var; // sysctl value, default = 2 // abc_val is 1 by default // ccv->nsegs is number of ACKs being aggregated due to LRO if (CCV(ccv, snd_nxt) == CCV(ccv, snd_max)) incr = min(ccv->bytes_this_ack, ccv->nsegs * abc_val * CCV(ccv, t_maxseg)); else incr = min(ccv->bytes_this_ack, CCV(ccv, t_maxseg)); } // incr == 2*1448 = 2896 in normal start start case As discussed in sec3.2 of RFC 3465, L=2*SMSS bytes exactly balances the negative impact of the delayed ACK algorithm. But if the receiver (sink) also does LRO, it won't generate enough ACKs to open cwnd of the sender. Often we observe that FreeBSD is slower when sending data using TCP, comparing to Linux or even Windows. https://lists.freebsd.org/archives/freebsd-hackers/2023-April/002082.html As suggested in https://calomel.org/freebsd_network_tuning.html , a large abc_l_var should help in this situation. # TCP Slow start gradually increases the data send rate until the TCP # congestion algorithm (CDG, H-TCP) calculates the networks maximum carrying # capacity without dropping packets. TCP Congestion Control with Appropriate # Byte Counting (ABC) allows our server to increase the maximum congestion # window exponentially by the amount of data ACKed, but limits the maximum # increment per ACK to (abc_l_var * maxseg) bytes. An abc_l_var of 44 times a # maxseg of 1460 bytes would allow slow start to increase the congestion window # by more than 64 kilobytes per step; 65535 bytes is the TCP receive buffer # size of most hosts without TCP window scaling. # net.inet.tcp.abc_l_var=44 # (default 2) if net.inet.tcp.mssdflt = 1460","title":"FreeBSD"},{"location":"slowstart/#traces","text":"When h_ertt is on, it will momentarily disable TSO whilst marking a packet to use for a new RTT measurement, resulting in more segments being sent, and more ACK received (from Linux), then cwnd could increase faster. Linux enters 'quickack' mode right after connection establishment, disabling delayed-ACKing for a while, to help peer measuring RTT and accelerate slow-start. But FreeBSD won't, it always delays ACKing (IIUC). // linux-stable/include/net/tcp.h /* Maximal number of ACKs sent quickly to accelerate slow-start. */ #define TCP_MAX_QUICKACKS 16U // linux-stable/net/ipv4/tcp_input.c static void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb) { // ... if (!icsk->icsk_ack.ato) { /* The _first_ data packet received, initialize * delayed ACK engine. */ tcp_incr_quickack(sk, TCP_MAX_QUICKACKS); icsk->icsk_ack.ato = TCP_ATO_MIN; FreeBSD 13 sender, Linux receiver Really slow start on a 100ms link: 0.000000 IP freebsd13 > linux: Flags [S], seq 205083268, win 65535, options [mss 1460,nop,wscale 10,sackOK,TS val 495212525 ecr 0], len 0 0.100786 IP linux > freebsd13: Flags [S.], seq 708257395, ack 205083269, win 65160, options [mss 1460,sackOK,TS val 563185696 ecr 495212525,nop,wscale 7], len 0 0.100800 IP freebsd13 > linux: Flags [.], ack 1, win 65, options [nop,nop,TS val 495212626 ecr 563185696], len 0 // cwnd = 10 0.101062 IP freebsd13 > linux: Flags [.], seq 1:14481, ack 1, win 65, len 14480 0.201241 IP linux > freebsd13: Flags [.], ack 14481, win 427, len 0 // cwnd = 12 0.201253 IP freebsd13 > linux: Flags [.], seq 14481:31857, ack 1, win 65, len 17376 0.301621 IP linux > freebsd13: Flags [.], ack 31857, win 411, len 0 // cwnd = 14 0.301630 IP freebsd13 > linux: Flags [.], seq 31857:52129, ack 1, win 65, len 20272 0.402010 IP linux > freebsd13: Flags [.], ack 52129, win 395, len 0 // cwnd = 16 0.402018 IP freebsd13 > linux: Flags [P.], seq 52129:73629, ack 1, win 65, len 21500 0.402026 IP freebsd13 > linux: Flags [.], seq 73629:75077, ack 1, win 65, len 1448 0.502392 IP linux > freebsd13: Flags [.], ack 75077, win 860, len 0 // cwnd = 18 0.502398 IP freebsd13 > linux: Flags [.], seq 75077:101141, ack 1, win 65, len 26064 0.602775 IP linux > freebsd13: Flags [.], ack 101141, win 1267, len 0 // cwnd = 20 0.602783 IP freebsd13 > linux: Flags [.], seq 101141:130101, ack 1, win 65, len 28960 0.703169 IP linux > freebsd13: Flags [.], ack 130101, win 1719, len 0 // cwnd = 22 0.703177 IP freebsd13 > linux: Flags [P.], seq 130101:159297, ack 1, win 65, len 29196 0.703185 IP freebsd13 > linux: Flags [.], seq 159297:160745, ack 1, win 65, len 1448 0.803367 IP linux > freebsd13: Flags [.], ack 160745, win 2198, len 0 // cwnd = 24 0.803375 IP freebsd13 > linux: Flags [.], seq 160745:195497, ack 1, win 65, len 34752 0.903754 IP linux > freebsd13: Flags [.], ack 195497, win 2741, len 0 // cwnd = 26 0.903762 IP freebsd13 > linux: Flags [.], seq 195497:233145, ack 1, win 65, len 37648 FreeBSD 14 sender, Linux receiver Faster than 13, because h_ertt is on by default. 0.000000 IP freebsd14 > linux: Flags [S], seq 3748224575, win 65535, options [mss 1460,nop,wscale 6,sackOK,TS val 790917200 ecr 0], len 0 0.100788 IP linux > freebsd14: Flags [S.], seq 489912744, ack 3748224576, win 65160, options [mss 1460,sackOK,TS val 380822116 ecr 790917200,nop,wscale 7], len 0 0.100851 IP freebsd14 > linux: Flags [.], ack 1, win 1027, options [nop,nop,TS val 790917303 ecr 380822116], len 0 // cwnd = 10 0.114635 IP freebsd14 > linux: Flags [.], seq 1:14481, ack 1, win 1027, len 14480 0.215265 IP linux > freebsd14: Flags [.], ack 14481, win 445, len 0 // cwnd = 12, h_ertt is on, send two segments, 1 MSS + 11 MSS 0.215334 IP freebsd14 > linux: Flags [.], seq 14481:15929, ack 1, win 1027, len 1448 0.215363 IP freebsd14 > linux: Flags [.], seq 15929:31857, ack 1, win 1027, len 15928 // got ACK for 1 MSS, cwnd = 13, send 2 segments 0.316142 IP linux > freebsd14: Flags [.], ack 15929, win 501, len 0 0.316210 IP freebsd14 > linux: Flags [.], seq 31857:33305, ack 1, win 1027, len 1448 0.316240 IP freebsd14 > linux: Flags [.], seq 33305:34753, ack 1, win 1027, len 1448 // got ACK for 11 MSS, cwnd = 15, send 1 segment of 13 MSS 0.316248 IP linux > freebsd14: Flags [.], ack 31857, win 440, len 0 0.316279 IP freebsd14 > linux: Flags [.], seq 34753:53577, ack 1, win 1027, len 18824 // got ACK for 1 MSS, cwnd = 16, send 2 segments 0.416970 IP linux > freebsd14: Flags [.], ack 33305, win 524, len 0 0.417047 IP freebsd14 > linux: Flags [.], seq 53577:55025, ack 1, win 1027, len 1448 0.417077 IP freebsd14 > linux: Flags [.], seq 55025:56473, ack 1, win 1027, len 1448 // got ACK for 1 MSS, cwnd = 17, send 1 segment of 2 MSS 0.417085 IP linux > freebsd14: Flags [.], ack 34753, win 546, len 0 0.417103 IP freebsd14 > linux: Flags [.], seq 56473:59369, ack 1, win 1027, len 2896 // got ACK for 13 MSS, cwnd = 19, send 1 segment of 15 MSS 0.417111 IP linux > freebsd14: Flags [.], ack 53577, win 840, len 0 0.417146 IP freebsd14 > linux: Flags [.], seq 59369:81089, ack 1, win 1027, len 21720 // And so on 0.517759 IP linux > freebsd14: Flags [.], ack 55025, win 863, len 0 0.517833 IP freebsd14 > linux: Flags [.], seq 81089:82537, ack 1, win 1027, len 1448 0.517862 IP freebsd14 > linux: Flags [.], seq 82537:83985, ack 1, win 1027, len 1448 FreeBSD 13 sender with h_ertt on, same as FreeBSD 14 above FreeBSD 14 sender, FreeBSD receiver Not as fast as Linux reciever, because the receiver delays ACKs.","title":"Traces"},{"location":"sockets/","text":"IPv6 Sockets Programming Notes struct sockaddr_in6 is bigger than struct sockaddr BSD Sockets API is not type-safe w.r.t. socket address structs, probably because it predates function signature checking in C. We often need to cast a struct sockaddr_in* or struct sockaddr_in6* to struct sockaddr* , like: // Sockets API often takes struct sockaddr*, which could point to // sockaddr_in, sockaddr_in6, or sockaddr_un. int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); // IPv4 struct sockaddr_in listen_addr = { .sin_family = AF_INET, .sin_port = htons(8000), .sin_addr.s_addr = htonl(INADDR_LOOPBACK), }; int sockfd = socket(AF_INET, SOCK_STREAM, 0); if (bind(sockfd, (struct sockaddr *) &listen_addr, sizeof listen_addr) < 0) { perror(\"bind\"); } // IPv6 struct sockaddr_in6 addr6 = { .sin6_family = AF_INET6, .sin6_port = htons(8000), .sin6_addr = in6addr_any, }; int sockfd = socket(AF_INET6, SOCK_STREAM, 0); if (bind(sockfd, (struct sockaddr *) &addr6, sizeof addr6) < 0) { perror(\"bind\"); } On platforms I tested, sizeof(struct sockaddr) == sizeof(struct sockaddr_in) == 16 , while sizeof(struct sockaddr_in6) == 28 . So sockets functions that take struct sockaddr* as input parameter usually work fine, e.g. bind() , connect() , sendto() . However, be careful about output parameter in accept() , recvfrom() , getpeername() , and getsockname() . Don't use struct sockaddr to hold the result, unless you know it's AF_INET not AF_INET6. Wrong UDP echo server for IPv6 void udp_echo(int sockfd) { struct sockaddr peerAddr; // *** Can't hold sockaddr_in6 socklen_t addrLen; char message[1024]; while (true) { addrLen = sizeof peerAddr; bzero(&peerAddr, sizeof peerAddr); ssize_t nr = recvfrom(sockfd, message, sizeof message, 0, &peerAddr, &addrLen); // peerAddr is truncated in IPv6, so message won't be echoed back to the sender. if (nr >= 0) { ssize_t nw = sendto(sockfd, message, nr, 0, &peerAddr, addrLen); } } } strace(1) output: recvfrom(3, \"hello\", 1024, 0, {sa_family=AF_INET6, sa_data=\"\\222R\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"}, [16->28]) = 5 sendto(3, \"hello\", 5, 0, {sa_family=AF_INET6, sin6_port=htons(37458), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::fa3b:3860:0:0\", &sin6_addr), sin6_scope_id=539754}, 28) = 5 Note that the address was truncated [16->28] in recvfrom() . Even worse, an address violation could happen when reading a `struct sockaddr` as `struct sockaddr_in6`. void printAddress(struct sockaddr* addr) { char buf[INET6_ADDRSTRLEN]; if (peerAddr.sa_family == AF_INET) { // ... } else if (peerAddr.sa_family == AF_INET6) { struct sockaddr_in6* addr6 = (struct sockaddr_in6*)&peerAddr; // *** if (inet_ntop(AF_INET6, &addr6->sin6_addr, buf, sizeof buf)) { printf(\"[%s]:%u\\n\", buf, ntohs(addr6->sin6_port)); } } } void printPeerNameWrong(int sockfd) { struct sockaddr peerAddr; socklen_t addrlen = sizeof peerAddr; if (getpeername(sockfd, &peerAddr, &addrlen) < 0) { perror(\"getpeername\"); return; } // !!! should check addrlen <= sizeof peerAddr. printAddress(&peerAddr); } AddressSanitizer output: ==3735==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffd8e5a3380 at pc 0x7f0e484164c2 bp 0x7ffd8e5a3240 sp 0x7ffd8e5a29f0 READ of size 16 at 0x7ffd8e5a3380 thread T0 #0 0x7f0e484164c1 in __interceptor_inet_ntop ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:2478 #1 0x5595a07923d1 in printAddress (/home/schen/cpp/a.out+0x13d1) #2 0x5595a0792728 in printPeerNameWrong (/home/schen/cpp/a.out+0x1728) #3 0x5595a0792b1c in main (/home/schen/cpp/a.out+0x1b1c) #4 0x7f0e48237d09 in __libc_start_main ../csu/libc-start.c:308 #5 0x5595a07921e9 in _start (/home/schen/cpp/a.out+0x11e9) Address 0x7ffd8e5a3380 is located in stack of thread T0 at offset 80 in frame #0 0x5595a0792645 in printPeerNameWrong (/home/schen/cpp/a.out+0x1645) This frame has 2 object(s): [48, 52) 'addrlen' (line 37) [64, 80) 'peerAddr' (line 36) <== Memory access at offset 80 overflows this variable SUMMARY: AddressSanitizer: stack-buffer-overflow ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:2478 in __interceptor_inet_ntop This stack buffer overflow is not detected by Valgrind, instead, the program prints some truncated address like 2601:647:4500:32bd:: when it should print 2601:647:4500:32bd:8a51:xxff:fexx:xxxx . In both cases above, struct sockaddr_storage should be used instead. A reinterpret_cast is usually needed in C++, or two static_cast from struct sockaddr_in* to void* , then to struct sockaddr* , like this: static_cast<sockaddr*>(static_cast<void*>(&addr)) . New functions like inet_pton and inet_ntop use void* for socket addresses, save us some casting. Also in C++, pass-by-reference ( struct sockaddr& ) vs. pass-by-value, the latter may be a victim of object-slicing problem. Host environment IPv4 only socket(AF_INET, ...) succeeds socket(AF_INET6, ...) fails IPv6 only socket(AF_INET, ...) fails socket(AF_INET6, ...) succeeds Dual-stack socket(AF_INET, ...) succeeds socket(AF_INET6, ...) succeeds Client Usually easy to write protocol-independent code. parse IP address & port from text, select sockaddr_in or sockaddr_in6 based on address format. create socket fd for saddr.sa_family. connect to saddr read/write as usual. Server IPv4 only, listen on 0.0.0.0:8000, serves IPv4 clients only. IPv6 dual-stack, listen on [::]:8000, serves both IPv4 and IPv6 clients. IPv4 clients show up as IPv4-mapped address, like ::ffff:10.0.0.118 . IPv6 only, listen on [::]:8000, turn on IPV6_V6ONLY , serves IPv6 clients only. Two sockets, one listens on [::]:8000, turn on IPV6_V6ONLY , another listens on 0.0.0.0:8000. Serves IPv6 and IPv4 clients, respectively. sshd and nginx do this by default. System wide IPV6_V6ONLY on Linux: sysctl net.ipv6.bindv6only /proc/sys/net/ipv6/bindv6only Linux has it off by default, but see ref below for different opinion from OpenBSD. read(2) vs. recv(2) Table 12.1 Sending and receiving data on a socket, from page 598 of The Design and Implementation of the FreeBSD Operating System, 2nd ed. . syscall Flags Address Scatter/Gather Aux Data read/write N N N N readv/writev N N Y N recv/send Y N N N recvfrom/sendto Y Y N N recvmsg/sendmsg Y Y Y Y On x86-64 Linux, recv(2) is not a syscall in fact, it's a wrapper of recvfrom(..., NULL, NULL) . Call graph as of Linux 5.15: G cluster_x net/socket.c sys_read __x64_sys_read __do_sys_read ksys_read vfs_read new_sync_read fs/read_write.c call_read_iter call_read_iter linux/fs.h sys_read->call_read_iter read() read() read()->sys_read sock_read_iter sock_read_iter call_read_iter->sock_read_iter sock_recvmsg sock_recvmsg sock_read_iter->sock_recvmsg sock_recvmsg_nosec sock_recvmsg_nosec sock_recvmsg->sock_recvmsg_nosec inet_recvmsg inet_recvmsg net/ipv4/af_inet.c sock_recvmsg_nosec->inet_recvmsg tcp_recvmsg tcp_recvmsg net/ipv4/tcp.c inet_recvmsg->tcp_recvmsg sys_recvmsg __x64_sys_recvmsg __se_sys_recvmsg __do_sys_recvmsg __sys_recvmsg net/socket.c sys_recvmsg->sock_recvmsg recvmsg() recvmsg() recvmsg()->sys_recvmsg sys_readv __x64_sys_readv __do_sys_readv do_readv vfs_readv do_iter_read do_iter_readv_writev fs/read_write.c sys_readv->call_read_iter readv() readv() readv()->sys_readv sys_recvfrom __x64_sys_recvfrom __se_sys_recvfrom __do_sys_recvfrom __sys_recvfrom net/socket.c sys_recvfrom->sock_recvmsg recvfrom() recvfrom() recvfrom()->sys_recvfrom recv() recv() __libc_recv\\nglibc on x86-64 __libc_recv glibc on x86-64 recv()->__libc_recv\\nglibc on x86-64 __libc_recv\\nglibc on x86-64->recvfrom() References RFC3493 Basic Socket Interface Extensions for IPv6, 2020/02 lwn688462 Should distributors disable IPv4-mapped IPv6? UNP3e Unix Network Programming 3/e , Chapter 12.","title":"Sockets"},{"location":"sockets/#ipv6-sockets-programming-notes","text":"","title":"IPv6 Sockets Programming Notes"},{"location":"sockets/#struct-sockaddr_in6-is-bigger-than-struct-sockaddr","text":"BSD Sockets API is not type-safe w.r.t. socket address structs, probably because it predates function signature checking in C. We often need to cast a struct sockaddr_in* or struct sockaddr_in6* to struct sockaddr* , like: // Sockets API often takes struct sockaddr*, which could point to // sockaddr_in, sockaddr_in6, or sockaddr_un. int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); // IPv4 struct sockaddr_in listen_addr = { .sin_family = AF_INET, .sin_port = htons(8000), .sin_addr.s_addr = htonl(INADDR_LOOPBACK), }; int sockfd = socket(AF_INET, SOCK_STREAM, 0); if (bind(sockfd, (struct sockaddr *) &listen_addr, sizeof listen_addr) < 0) { perror(\"bind\"); } // IPv6 struct sockaddr_in6 addr6 = { .sin6_family = AF_INET6, .sin6_port = htons(8000), .sin6_addr = in6addr_any, }; int sockfd = socket(AF_INET6, SOCK_STREAM, 0); if (bind(sockfd, (struct sockaddr *) &addr6, sizeof addr6) < 0) { perror(\"bind\"); } On platforms I tested, sizeof(struct sockaddr) == sizeof(struct sockaddr_in) == 16 , while sizeof(struct sockaddr_in6) == 28 . So sockets functions that take struct sockaddr* as input parameter usually work fine, e.g. bind() , connect() , sendto() . However, be careful about output parameter in accept() , recvfrom() , getpeername() , and getsockname() . Don't use struct sockaddr to hold the result, unless you know it's AF_INET not AF_INET6. Wrong UDP echo server for IPv6 void udp_echo(int sockfd) { struct sockaddr peerAddr; // *** Can't hold sockaddr_in6 socklen_t addrLen; char message[1024]; while (true) { addrLen = sizeof peerAddr; bzero(&peerAddr, sizeof peerAddr); ssize_t nr = recvfrom(sockfd, message, sizeof message, 0, &peerAddr, &addrLen); // peerAddr is truncated in IPv6, so message won't be echoed back to the sender. if (nr >= 0) { ssize_t nw = sendto(sockfd, message, nr, 0, &peerAddr, addrLen); } } } strace(1) output: recvfrom(3, \"hello\", 1024, 0, {sa_family=AF_INET6, sa_data=\"\\222R\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"}, [16->28]) = 5 sendto(3, \"hello\", 5, 0, {sa_family=AF_INET6, sin6_port=htons(37458), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::fa3b:3860:0:0\", &sin6_addr), sin6_scope_id=539754}, 28) = 5 Note that the address was truncated [16->28] in recvfrom() . Even worse, an address violation could happen when reading a `struct sockaddr` as `struct sockaddr_in6`. void printAddress(struct sockaddr* addr) { char buf[INET6_ADDRSTRLEN]; if (peerAddr.sa_family == AF_INET) { // ... } else if (peerAddr.sa_family == AF_INET6) { struct sockaddr_in6* addr6 = (struct sockaddr_in6*)&peerAddr; // *** if (inet_ntop(AF_INET6, &addr6->sin6_addr, buf, sizeof buf)) { printf(\"[%s]:%u\\n\", buf, ntohs(addr6->sin6_port)); } } } void printPeerNameWrong(int sockfd) { struct sockaddr peerAddr; socklen_t addrlen = sizeof peerAddr; if (getpeername(sockfd, &peerAddr, &addrlen) < 0) { perror(\"getpeername\"); return; } // !!! should check addrlen <= sizeof peerAddr. printAddress(&peerAddr); } AddressSanitizer output: ==3735==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffd8e5a3380 at pc 0x7f0e484164c2 bp 0x7ffd8e5a3240 sp 0x7ffd8e5a29f0 READ of size 16 at 0x7ffd8e5a3380 thread T0 #0 0x7f0e484164c1 in __interceptor_inet_ntop ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:2478 #1 0x5595a07923d1 in printAddress (/home/schen/cpp/a.out+0x13d1) #2 0x5595a0792728 in printPeerNameWrong (/home/schen/cpp/a.out+0x1728) #3 0x5595a0792b1c in main (/home/schen/cpp/a.out+0x1b1c) #4 0x7f0e48237d09 in __libc_start_main ../csu/libc-start.c:308 #5 0x5595a07921e9 in _start (/home/schen/cpp/a.out+0x11e9) Address 0x7ffd8e5a3380 is located in stack of thread T0 at offset 80 in frame #0 0x5595a0792645 in printPeerNameWrong (/home/schen/cpp/a.out+0x1645) This frame has 2 object(s): [48, 52) 'addrlen' (line 37) [64, 80) 'peerAddr' (line 36) <== Memory access at offset 80 overflows this variable SUMMARY: AddressSanitizer: stack-buffer-overflow ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:2478 in __interceptor_inet_ntop This stack buffer overflow is not detected by Valgrind, instead, the program prints some truncated address like 2601:647:4500:32bd:: when it should print 2601:647:4500:32bd:8a51:xxff:fexx:xxxx . In both cases above, struct sockaddr_storage should be used instead. A reinterpret_cast is usually needed in C++, or two static_cast from struct sockaddr_in* to void* , then to struct sockaddr* , like this: static_cast<sockaddr*>(static_cast<void*>(&addr)) . New functions like inet_pton and inet_ntop use void* for socket addresses, save us some casting. Also in C++, pass-by-reference ( struct sockaddr& ) vs. pass-by-value, the latter may be a victim of object-slicing problem.","title":"struct sockaddr_in6 is bigger than struct sockaddr"},{"location":"sockets/#host-environment","text":"IPv4 only socket(AF_INET, ...) succeeds socket(AF_INET6, ...) fails IPv6 only socket(AF_INET, ...) fails socket(AF_INET6, ...) succeeds Dual-stack socket(AF_INET, ...) succeeds socket(AF_INET6, ...) succeeds","title":"Host environment"},{"location":"sockets/#client","text":"Usually easy to write protocol-independent code. parse IP address & port from text, select sockaddr_in or sockaddr_in6 based on address format. create socket fd for saddr.sa_family. connect to saddr read/write as usual.","title":"Client"},{"location":"sockets/#server","text":"IPv4 only, listen on 0.0.0.0:8000, serves IPv4 clients only. IPv6 dual-stack, listen on [::]:8000, serves both IPv4 and IPv6 clients. IPv4 clients show up as IPv4-mapped address, like ::ffff:10.0.0.118 . IPv6 only, listen on [::]:8000, turn on IPV6_V6ONLY , serves IPv6 clients only. Two sockets, one listens on [::]:8000, turn on IPV6_V6ONLY , another listens on 0.0.0.0:8000. Serves IPv6 and IPv4 clients, respectively. sshd and nginx do this by default. System wide IPV6_V6ONLY on Linux: sysctl net.ipv6.bindv6only /proc/sys/net/ipv6/bindv6only Linux has it off by default, but see ref below for different opinion from OpenBSD.","title":"Server"},{"location":"sockets/#read2-vs-recv2","text":"Table 12.1 Sending and receiving data on a socket, from page 598 of The Design and Implementation of the FreeBSD Operating System, 2nd ed. . syscall Flags Address Scatter/Gather Aux Data read/write N N N N readv/writev N N Y N recv/send Y N N N recvfrom/sendto Y Y N N recvmsg/sendmsg Y Y Y Y On x86-64 Linux, recv(2) is not a syscall in fact, it's a wrapper of recvfrom(..., NULL, NULL) . Call graph as of Linux 5.15: G cluster_x net/socket.c sys_read __x64_sys_read __do_sys_read ksys_read vfs_read new_sync_read fs/read_write.c call_read_iter call_read_iter linux/fs.h sys_read->call_read_iter read() read() read()->sys_read sock_read_iter sock_read_iter call_read_iter->sock_read_iter sock_recvmsg sock_recvmsg sock_read_iter->sock_recvmsg sock_recvmsg_nosec sock_recvmsg_nosec sock_recvmsg->sock_recvmsg_nosec inet_recvmsg inet_recvmsg net/ipv4/af_inet.c sock_recvmsg_nosec->inet_recvmsg tcp_recvmsg tcp_recvmsg net/ipv4/tcp.c inet_recvmsg->tcp_recvmsg sys_recvmsg __x64_sys_recvmsg __se_sys_recvmsg __do_sys_recvmsg __sys_recvmsg net/socket.c sys_recvmsg->sock_recvmsg recvmsg() recvmsg() recvmsg()->sys_recvmsg sys_readv __x64_sys_readv __do_sys_readv do_readv vfs_readv do_iter_read do_iter_readv_writev fs/read_write.c sys_readv->call_read_iter readv() readv() readv()->sys_readv sys_recvfrom __x64_sys_recvfrom __se_sys_recvfrom __do_sys_recvfrom __sys_recvfrom net/socket.c sys_recvfrom->sock_recvmsg recvfrom() recvfrom() recvfrom()->sys_recvfrom recv() recv() __libc_recv\\nglibc on x86-64 __libc_recv glibc on x86-64 recv()->__libc_recv\\nglibc on x86-64 __libc_recv\\nglibc on x86-64->recvfrom()","title":"read(2) vs. recv(2)"},{"location":"sockets/#references","text":"RFC3493 Basic Socket Interface Extensions for IPv6, 2020/02 lwn688462 Should distributors disable IPv4-mapped IPv6? UNP3e Unix Network Programming 3/e , Chapter 12.","title":"References"},{"location":"throughput/","text":"TCP Throughput TCP Throughput <= Bytes in flight / RTT, where RTT = round-trip time. Max bytes in flight = min(Cwnd, Rwnd, sndbuf, BDP). TCP trace segment graph tcptrace is a tool written by Shawn Ostermann at Ohio University, http://tcptrace.org. Wireshark can produce nice interactive graphs. Here we show FreeBSD's NewReno congestion control, slow start after packet loss. Mininet All graphs below are from Linux using CUBIC cc, running under Mininet . mininet> net h1 h1-eth0:s1-eth1 h2 h2-eth0:s1-eth2 s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0 Packets are captured at sender side tcpdump -i s1-eth1 -s 128 . mininet> h1 ping -c 4 h2 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.252 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.053 ms 64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.059 ms 64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=0.056 ms mininet> xterm h2 # then start `iperf3 -s` on h2 mininet> h1 iperf3 -c h2 Connecting to host 10.0.0.2, port 5201 [ 5] local 10.0.0.1 port 37680 connected to 10.0.0.2 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 8.00 GBytes 68.7 Gbits/sec 0 666 KBytes [ 5] 1.00-2.00 sec 8.20 GBytes 70.4 Gbits/sec 0 940 KBytes [ 5] 2.00-3.00 sec 7.89 GBytes 67.7 Gbits/sec 0 1.15 MBytes [ 5] 3.00-4.00 sec 7.90 GBytes 67.9 Gbits/sec 0 1.28 MBytes [ 5] 4.00-5.00 sec 7.98 GBytes 68.6 Gbits/sec 0 1.42 MBytes [ 5] 5.00-6.00 sec 8.16 GBytes 70.1 Gbits/sec 0 1.70 MBytes [ 5] 6.00-7.00 sec 8.17 GBytes 70.2 Gbits/sec 0 1.70 MBytes [ 5] 7.00-8.00 sec 8.14 GBytes 69.9 Gbits/sec 0 1.79 MBytes [ 5] 8.00-9.00 sec 8.04 GBytes 69.0 Gbits/sec 0 1.79 MBytes [ 5] 9.00-10.00 sec 7.99 GBytes 68.7 Gbits/sec 0 1.88 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 80.5 GBytes 69.1 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 80.5 GBytes 69.1 Gbits/sec receiver iperf Done. Add 100ms latency using netem delay 100ms . mininet> s1 tc qdisc replace dev s1-eth2 root netem delay 100ms mininet> h1 ping -c 4 h2 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=101 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=100 ms 64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=100 ms 64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=100 ms Slow sender iperf3 -c server --bitrate 10M Slow sender using FQ pacing iperf3 -c server --fq-rate 10M No bursts. Slow receiver iperf3 -s --server-bitrate-limit 10M Small window size. Small Rwnd mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:37662 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 85.3Ki 2048Mi 201.2ms/100593 1.048s 21.0MB/s 168Mbps 10.5Mi 6094Ki 16.0Mi 2048Mi 100.2ms/56 retrans=3 2.050s 65.2MB/s 522Mbps 16.5Mi 6518Ki 16.0Mi 2048Mi 100.4ms/107 3.054s 66.6MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.4ms/80 4.058s 66.7MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.5ms/61 5.063s 66.7MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.5ms/80 6.066s 66.8MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.4ms/81 7.070s 66.6MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.4ms/69 8.074s 66.8MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.4ms/68 9.077s 66.9MB/s 535Mbps 16.5Mi 6552Ki 16.0Mi 2048Mi 100.3ms/77 10.081s 66.7MB/s 534Mbps 16.5Mi 6548Ki 16.0Mi 2048Mi 100.4ms/115 Transferred 623MBytes in 10.238s, 4754 syscalls, 131072.0 Bytes/syscall Throughput is limited by Rwnd ( snd_wnd ), 100ms * 66.7MB/s = 6.5MB. Window is filled up as soon as advertised. Set larger tcp_rmem on receiver, for larger Rwnd. mininet> h2 sysctl -A |grep tcp_.mem net.ipv4.tcp_rmem = 10240 87380 16777216 net.ipv4.tcp_wmem = 10240 87380 16777216 mininet> h2 sysctl -A |grep tcp_adv net.ipv4.tcp_adv_win_scale = 1 mininet> h2 sysctl -w net.ipv4.tcp_rmem=\"10240 131072 65536000\" net.ipv4.tcp_rmem = 10240 131072 65536000 For net.ipv4.tcp_adv_win_scale = 1 , Rwnd = tcp_rmem[2] / 2 = 32MB . Small sndbuf mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:45092 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 85.3Ki 2048Mi 100.7ms/50326 1.045s 18.3MB/s 147Mbps 10.2Mi 8474Ki 16.0Mi 2048Mi 103.6ms/6137 retrans=2 2.034s 148MB/s 1184Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/20 3.014s 159MB/s 1272Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/1 4.005s 157MB/s 1255Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/5 5.042s 158MB/s 1264Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/16 6.020s 159MB/s 1275Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/15 7.016s 154MB/s 1229Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/2 8.055s 158MB/s 1261Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/0 9.019s 164MB/s 1314Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/15 10.034s 151MB/s 1206Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/14 Transferred 1425MBytes in 10.134s, 10869 syscalls, 131072.0 Bytes/syscall Throughput is limited by sndbuf, 100ms * 160MB/s = 16MB. Rwnd is only half filled, in a burst. Higher throughput achived by larger sndbuf . mininet> h1 sysctl -w net.ipv4.tcp_wmem=\"10240 131072 65536000\" net.ipv4.tcp_wmem = 10240 131072 65536000 mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:53176 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 128Ki 2048Mi 100.4ms/50205 1.004s 32.1MB/s 257Mbps 7767Ki 11.0Mi 37.8Mi 2048Mi 100.5ms/47 2.051s 249MB/s 1995Mbps 68.3Mi 24.9Mi 62.5Mi 2048Mi 100.7ms/134 retrans=585 3.056s 260MB/s 2081Mbps 68.3Mi 24.9Mi 62.5Mi 2048Mi 100.6ms/147 4.061s 260MB/s 2082Mbps 68.3Mi 21.9Mi 62.5Mi 2048Mi 100.5ms/23 5.024s 274MB/s 2194Mbps 68.3Mi 24.3Mi 62.5Mi 2048Mi 100.2ms/64 6.028s 313MB/s 2500Mbps 68.3Mi 26.4Mi 62.5Mi 2048Mi 100.1ms/58 7.034s 325MB/s 2603Mbps 68.3Mi 29.6Mi 62.5Mi 2048Mi 100.1ms/73 8.040s 325MB/s 2602Mbps 68.3Mi 28.9Mi 62.5Mi 2048Mi 100.1ms/40 9.046s 326MB/s 2604Mbps 68.3Mi 29.7Mi 62.5Mi 2048Mi 100.2ms/112 10.051s 325MB/s 2603Mbps 68.3Mi 27.9Mi 62.5Mi 2048Mi 100.1ms/100 Transferred 2703MBytes in 10.154s, 20625 syscalls, 131072.0 Bytes/syscall Small Cwnd Congestion control algorithms decide congestion window (Cwnd). With RTT = 100ms, FreeBSD newreno CC sometimes increases Cwnd slowly. In the following example, reaches max bandwidth after ~30 seconds. freebsd:~/recipes/tpc % bin/tcpperf -c 172.16.0.59 -b 100G -t 30 Connected 172.16.0.77:31839 -> 172.16.0.59:2009, congestion control: newreno Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 63.6Ki 32.8Ki 1024Mi 202.0ms/101000 1.104s 356kB/s 2849kbps 45.5Ki 435Ki 96.8Ki 1024Mi 123.8ms/42562 2.007s 580kB/s 4643kbps 71.2Ki 589Ki 137Ki 1024Mi 107.5ms/13937 3.010s 915kB/s 7318kbps 99.7Ki 593Ki 201Ki 1024Mi 102.3ms/3750 4.014s 1176kB/s 9404kbps 128Ki 593Ki 257Ki 1024Mi 100.9ms/1187 5.017s 1437kB/s 11.5Mbps 162Ki 1307Ki 321Ki 1024Mi 100.5ms/687 6.021s 1829kB/s 14.6Mbps 191Ki 1307Ki 377Ki 1024Mi 100.4ms/562 7.024s 2090kB/s 16.7Mbps 219Ki 1287Ki 441Ki 1024Mi 100.4ms/687 8.028s 2481kB/s 19.9Mbps 248Ki 1266Ki 497Ki 1024Mi 100.5ms/687 9.032s 2611kB/s 20.9Mbps 276Ki 1246Ki 553Ki 1024Mi 100.5ms/750 10.035s 3005kB/s 24.0Mbps 311Ki 1820Ki 617Ki 1024Mi 100.4ms/500 11.038s 3396kB/s 27.2Mbps 339Ki 1894Ki 681Ki 1024Mi 100.4ms/500 12.042s 3526kB/s 28.2Mbps 368Ki 1919Ki 737Ki 1024Mi 100.5ms/625 13.047s 4045kB/s 32.4Mbps 418Ki 1852Ki 857Ki 1024Mi 101.2ms/1375 14.050s 4701kB/s 37.6Mbps 480Ki 1868Ki 1009Ki 1024Mi 100.4ms/562 15.054s 5354kB/s 42.8Mbps 553Ki 2361Ki 1169Ki 1024Mi 101.3ms/2000 16.039s 6255kB/s 50.0Mbps 641Ki 2681Ki 1377Ki 1024Mi 100.6ms/687 17.002s 7212kB/s 57.7Mbps 727Ki 2696Ki 1561Ki 1024Mi 102.2ms/4062 18.006s 8098kB/s 64.8Mbps 813Ki 2684Ki 1769Ki 1024Mi 100.7ms/687 19.010s 8617kB/s 68.9Mbps 898Ki 2663Ki 1817Ki 1024Mi 100.5ms/625 20.013s 9662kB/s 77.3Mbps 1004Ki 2914Ki 1817Ki 1024Mi 100.4ms/500 21.017s 10.7MB/s 85.7Mbps 1118Ki 2893Ki 1817Ki 1024Mi 100.4ms/500 22.021s 11.9MB/s 95.0Mbps 1235Ki 3010Ki 1817Ki 1024Mi 100.4ms/500 23.025s 13.2MB/s 106Mbps 1369Ki 2989Ki 1817Ki 1024Mi 100.3ms/437 24.029s 14.5MB/s 116Mbps 1497Ki 2916Ki 1817Ki 1024Mi 100.5ms/687 25.033s 15.9MB/s 127Mbps 1634Ki 2930Ki 1817Ki 1024Mi 100.3ms/437 26.037s 17.2MB/s 138Mbps 1796Ki 3030Ki 1817Ki 1024Mi 100.4ms/562 27.001s 18.2MB/s 146Mbps 2010Ki 3066Ki 1817Ki 1024Mi 100.8ms/687 28.005s 18.5MB/s 148Mbps 2191Ki 3026Ki 1817Ki 1024Mi 100.7ms/687 29.008s 18.4MB/s 147Mbps 2419Ki 2986Ki 1817Ki 1024Mi 100.4ms/562 30.012s 18.5MB/s 148Mbps 2619Ki 2992Ki 1817Ki 1024Mi 100.7ms/625 Transferred 234MBytes in 30.113s, 1787 syscalls, 131072.0 Bytes/syscall Before Cwnd reaches sndbuf (1.8MB), throughput is dominated by Cwnd. e.g. at 20-th second: Cwnd = 1000K, throughput = 1000K / 0.1s = 10MB/s. After that, throughput is domnated by sndbuf in this case. e.g. at 30-th second, sndbuf = 1.8MB, throughput = 18MB/s. See discussion on freebsd-net mailing list 2023-05: https://lists.freebsd.org/archives/freebsd-net/2023-May/003282.html and Slow Start Bandwidth limit mininet> s1 tc qdisc replace dev s1-eth2 root netem delay 10ms rate 10Mbit mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:48982 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 128Ki 2048Mi 10.7ms/5341 1.120s 1638kB/s 13.1Mbps 132Ki 323Ki 790Ki 70.7Ki 106.2ms/574 2.094s 1481kB/s 11.8Mbps 189Ki 416Ki 1139Ki 70.7Ki 152.8ms/1331 3.018s 1418kB/s 11.3Mbps 243Ki 535Ki 1462Ki 70.7Ki 196.7ms/1030 4.226s 1411kB/s 11.3Mbps 314Ki 680Ki 1887Ki 70.7Ki 254.5ms/1929 5.196s 1351kB/s 10.8Mbps 370Ki 795Ki 2227Ki 70.7Ki 300.2ms/813 6.357s 1354kB/s 10.8Mbps 438Ki 955Ki 2635Ki 70.7Ki 355.6ms/856 7.009s 1408kB/s 11.3Mbps 475Ki 1021Ki 2856Ki 70.7Ki 386.2ms/374 8.444s 1461kB/s 11.7Mbps 560Ki 1213Ki 3366Ki 70.7Ki 455.5ms/1334 9.281s 1409kB/s 11.3Mbps 608Ki 1326Ki 3655Ki 70.7Ki 494.9ms/801 10.160s 1342kB/s 10.7Mbps 660Ki 1399Ki 3970Ki 70.7Ki 537.0ms/1078 Transferred 14.5MBytes in 12.180s, 111 syscalls, 131072.0 Bytes/syscall This is probably the best case, as all availabe bandwidth is utilized. Performance According to Understanding Host Network Stack Overheads SIGCOMM'21, modern Linux network stack can achieve ~42Gbps throughput-per-core . In other words, a single TCP connection can sustain a 40Gbps NIC unidirectionally, but not an 100Gbps NIC. Eric reported 170Gbps single flow on 200Gbps NIC in netdev conf 2022-10, with receiver zero copy and BIG TCP .","title":"Throughput"},{"location":"throughput/#tcp-throughput","text":"TCP Throughput <= Bytes in flight / RTT, where RTT = round-trip time. Max bytes in flight = min(Cwnd, Rwnd, sndbuf, BDP).","title":"TCP Throughput"},{"location":"throughput/#tcp-trace-segment-graph","text":"tcptrace is a tool written by Shawn Ostermann at Ohio University, http://tcptrace.org. Wireshark can produce nice interactive graphs. Here we show FreeBSD's NewReno congestion control, slow start after packet loss.","title":"TCP trace segment graph"},{"location":"throughput/#mininet","text":"All graphs below are from Linux using CUBIC cc, running under Mininet . mininet> net h1 h1-eth0:s1-eth1 h2 h2-eth0:s1-eth2 s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0 Packets are captured at sender side tcpdump -i s1-eth1 -s 128 . mininet> h1 ping -c 4 h2 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.252 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.053 ms 64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=0.059 ms 64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=0.056 ms mininet> xterm h2 # then start `iperf3 -s` on h2 mininet> h1 iperf3 -c h2 Connecting to host 10.0.0.2, port 5201 [ 5] local 10.0.0.1 port 37680 connected to 10.0.0.2 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 8.00 GBytes 68.7 Gbits/sec 0 666 KBytes [ 5] 1.00-2.00 sec 8.20 GBytes 70.4 Gbits/sec 0 940 KBytes [ 5] 2.00-3.00 sec 7.89 GBytes 67.7 Gbits/sec 0 1.15 MBytes [ 5] 3.00-4.00 sec 7.90 GBytes 67.9 Gbits/sec 0 1.28 MBytes [ 5] 4.00-5.00 sec 7.98 GBytes 68.6 Gbits/sec 0 1.42 MBytes [ 5] 5.00-6.00 sec 8.16 GBytes 70.1 Gbits/sec 0 1.70 MBytes [ 5] 6.00-7.00 sec 8.17 GBytes 70.2 Gbits/sec 0 1.70 MBytes [ 5] 7.00-8.00 sec 8.14 GBytes 69.9 Gbits/sec 0 1.79 MBytes [ 5] 8.00-9.00 sec 8.04 GBytes 69.0 Gbits/sec 0 1.79 MBytes [ 5] 9.00-10.00 sec 7.99 GBytes 68.7 Gbits/sec 0 1.88 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 80.5 GBytes 69.1 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 80.5 GBytes 69.1 Gbits/sec receiver iperf Done. Add 100ms latency using netem delay 100ms . mininet> s1 tc qdisc replace dev s1-eth2 root netem delay 100ms mininet> h1 ping -c 4 h2 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=101 ms 64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=100 ms 64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=100 ms 64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=100 ms","title":"Mininet"},{"location":"throughput/#slow-sender","text":"iperf3 -c server --bitrate 10M","title":"Slow sender"},{"location":"throughput/#slow-sender-using-fq-pacing","text":"iperf3 -c server --fq-rate 10M No bursts.","title":"Slow sender using FQ pacing"},{"location":"throughput/#slow-receiver","text":"iperf3 -s --server-bitrate-limit 10M Small window size.","title":"Slow receiver"},{"location":"throughput/#small-rwnd","text":"mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:37662 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 85.3Ki 2048Mi 201.2ms/100593 1.048s 21.0MB/s 168Mbps 10.5Mi 6094Ki 16.0Mi 2048Mi 100.2ms/56 retrans=3 2.050s 65.2MB/s 522Mbps 16.5Mi 6518Ki 16.0Mi 2048Mi 100.4ms/107 3.054s 66.6MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.4ms/80 4.058s 66.7MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.5ms/61 5.063s 66.7MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.5ms/80 6.066s 66.8MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.4ms/81 7.070s 66.6MB/s 533Mbps 16.5Mi 6546Ki 16.0Mi 2048Mi 100.4ms/69 8.074s 66.8MB/s 534Mbps 16.5Mi 6520Ki 16.0Mi 2048Mi 100.4ms/68 9.077s 66.9MB/s 535Mbps 16.5Mi 6552Ki 16.0Mi 2048Mi 100.3ms/77 10.081s 66.7MB/s 534Mbps 16.5Mi 6548Ki 16.0Mi 2048Mi 100.4ms/115 Transferred 623MBytes in 10.238s, 4754 syscalls, 131072.0 Bytes/syscall Throughput is limited by Rwnd ( snd_wnd ), 100ms * 66.7MB/s = 6.5MB. Window is filled up as soon as advertised. Set larger tcp_rmem on receiver, for larger Rwnd. mininet> h2 sysctl -A |grep tcp_.mem net.ipv4.tcp_rmem = 10240 87380 16777216 net.ipv4.tcp_wmem = 10240 87380 16777216 mininet> h2 sysctl -A |grep tcp_adv net.ipv4.tcp_adv_win_scale = 1 mininet> h2 sysctl -w net.ipv4.tcp_rmem=\"10240 131072 65536000\" net.ipv4.tcp_rmem = 10240 131072 65536000 For net.ipv4.tcp_adv_win_scale = 1 , Rwnd = tcp_rmem[2] / 2 = 32MB .","title":"Small Rwnd"},{"location":"throughput/#small-sndbuf","text":"mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:45092 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 85.3Ki 2048Mi 100.7ms/50326 1.045s 18.3MB/s 147Mbps 10.2Mi 8474Ki 16.0Mi 2048Mi 103.6ms/6137 retrans=2 2.034s 148MB/s 1184Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/20 3.014s 159MB/s 1272Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/1 4.005s 157MB/s 1255Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/5 5.042s 158MB/s 1264Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/16 6.020s 159MB/s 1275Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/15 7.016s 154MB/s 1229Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/2 8.055s 158MB/s 1261Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/0 9.019s 164MB/s 1314Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/15 10.034s 151MB/s 1206Mbps 31.4Mi 31.2Mi 16.0Mi 2048Mi 100.0ms/14 Transferred 1425MBytes in 10.134s, 10869 syscalls, 131072.0 Bytes/syscall Throughput is limited by sndbuf, 100ms * 160MB/s = 16MB. Rwnd is only half filled, in a burst. Higher throughput achived by larger sndbuf . mininet> h1 sysctl -w net.ipv4.tcp_wmem=\"10240 131072 65536000\" net.ipv4.tcp_wmem = 10240 131072 65536000 mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:53176 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 128Ki 2048Mi 100.4ms/50205 1.004s 32.1MB/s 257Mbps 7767Ki 11.0Mi 37.8Mi 2048Mi 100.5ms/47 2.051s 249MB/s 1995Mbps 68.3Mi 24.9Mi 62.5Mi 2048Mi 100.7ms/134 retrans=585 3.056s 260MB/s 2081Mbps 68.3Mi 24.9Mi 62.5Mi 2048Mi 100.6ms/147 4.061s 260MB/s 2082Mbps 68.3Mi 21.9Mi 62.5Mi 2048Mi 100.5ms/23 5.024s 274MB/s 2194Mbps 68.3Mi 24.3Mi 62.5Mi 2048Mi 100.2ms/64 6.028s 313MB/s 2500Mbps 68.3Mi 26.4Mi 62.5Mi 2048Mi 100.1ms/58 7.034s 325MB/s 2603Mbps 68.3Mi 29.6Mi 62.5Mi 2048Mi 100.1ms/73 8.040s 325MB/s 2602Mbps 68.3Mi 28.9Mi 62.5Mi 2048Mi 100.1ms/40 9.046s 326MB/s 2604Mbps 68.3Mi 29.7Mi 62.5Mi 2048Mi 100.2ms/112 10.051s 325MB/s 2603Mbps 68.3Mi 27.9Mi 62.5Mi 2048Mi 100.1ms/100 Transferred 2703MBytes in 10.154s, 20625 syscalls, 131072.0 Bytes/syscall","title":"Small sndbuf"},{"location":"throughput/#small-cwnd","text":"Congestion control algorithms decide congestion window (Cwnd). With RTT = 100ms, FreeBSD newreno CC sometimes increases Cwnd slowly. In the following example, reaches max bandwidth after ~30 seconds. freebsd:~/recipes/tpc % bin/tcpperf -c 172.16.0.59 -b 100G -t 30 Connected 172.16.0.77:31839 -> 172.16.0.59:2009, congestion control: newreno Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 63.6Ki 32.8Ki 1024Mi 202.0ms/101000 1.104s 356kB/s 2849kbps 45.5Ki 435Ki 96.8Ki 1024Mi 123.8ms/42562 2.007s 580kB/s 4643kbps 71.2Ki 589Ki 137Ki 1024Mi 107.5ms/13937 3.010s 915kB/s 7318kbps 99.7Ki 593Ki 201Ki 1024Mi 102.3ms/3750 4.014s 1176kB/s 9404kbps 128Ki 593Ki 257Ki 1024Mi 100.9ms/1187 5.017s 1437kB/s 11.5Mbps 162Ki 1307Ki 321Ki 1024Mi 100.5ms/687 6.021s 1829kB/s 14.6Mbps 191Ki 1307Ki 377Ki 1024Mi 100.4ms/562 7.024s 2090kB/s 16.7Mbps 219Ki 1287Ki 441Ki 1024Mi 100.4ms/687 8.028s 2481kB/s 19.9Mbps 248Ki 1266Ki 497Ki 1024Mi 100.5ms/687 9.032s 2611kB/s 20.9Mbps 276Ki 1246Ki 553Ki 1024Mi 100.5ms/750 10.035s 3005kB/s 24.0Mbps 311Ki 1820Ki 617Ki 1024Mi 100.4ms/500 11.038s 3396kB/s 27.2Mbps 339Ki 1894Ki 681Ki 1024Mi 100.4ms/500 12.042s 3526kB/s 28.2Mbps 368Ki 1919Ki 737Ki 1024Mi 100.5ms/625 13.047s 4045kB/s 32.4Mbps 418Ki 1852Ki 857Ki 1024Mi 101.2ms/1375 14.050s 4701kB/s 37.6Mbps 480Ki 1868Ki 1009Ki 1024Mi 100.4ms/562 15.054s 5354kB/s 42.8Mbps 553Ki 2361Ki 1169Ki 1024Mi 101.3ms/2000 16.039s 6255kB/s 50.0Mbps 641Ki 2681Ki 1377Ki 1024Mi 100.6ms/687 17.002s 7212kB/s 57.7Mbps 727Ki 2696Ki 1561Ki 1024Mi 102.2ms/4062 18.006s 8098kB/s 64.8Mbps 813Ki 2684Ki 1769Ki 1024Mi 100.7ms/687 19.010s 8617kB/s 68.9Mbps 898Ki 2663Ki 1817Ki 1024Mi 100.5ms/625 20.013s 9662kB/s 77.3Mbps 1004Ki 2914Ki 1817Ki 1024Mi 100.4ms/500 21.017s 10.7MB/s 85.7Mbps 1118Ki 2893Ki 1817Ki 1024Mi 100.4ms/500 22.021s 11.9MB/s 95.0Mbps 1235Ki 3010Ki 1817Ki 1024Mi 100.4ms/500 23.025s 13.2MB/s 106Mbps 1369Ki 2989Ki 1817Ki 1024Mi 100.3ms/437 24.029s 14.5MB/s 116Mbps 1497Ki 2916Ki 1817Ki 1024Mi 100.5ms/687 25.033s 15.9MB/s 127Mbps 1634Ki 2930Ki 1817Ki 1024Mi 100.3ms/437 26.037s 17.2MB/s 138Mbps 1796Ki 3030Ki 1817Ki 1024Mi 100.4ms/562 27.001s 18.2MB/s 146Mbps 2010Ki 3066Ki 1817Ki 1024Mi 100.8ms/687 28.005s 18.5MB/s 148Mbps 2191Ki 3026Ki 1817Ki 1024Mi 100.7ms/687 29.008s 18.4MB/s 147Mbps 2419Ki 2986Ki 1817Ki 1024Mi 100.4ms/562 30.012s 18.5MB/s 148Mbps 2619Ki 2992Ki 1817Ki 1024Mi 100.7ms/625 Transferred 234MBytes in 30.113s, 1787 syscalls, 131072.0 Bytes/syscall Before Cwnd reaches sndbuf (1.8MB), throughput is dominated by Cwnd. e.g. at 20-th second: Cwnd = 1000K, throughput = 1000K / 0.1s = 10MB/s. After that, throughput is domnated by sndbuf in this case. e.g. at 30-th second, sndbuf = 1.8MB, throughput = 18MB/s. See discussion on freebsd-net mailing list 2023-05: https://lists.freebsd.org/archives/freebsd-net/2023-May/003282.html and Slow Start","title":"Small Cwnd"},{"location":"throughput/#bandwidth-limit","text":"mininet> s1 tc qdisc replace dev s1-eth2 root netem delay 10ms rate 10Mbit mininet> h1 bin/tcpperf -c h2 Connected 10.0.0.1:48982 -> 10.0.0.2:2009, congestion control: cubic Time (s) Throughput Bitrate Cwnd Rwnd sndbuf ssthresh rtt/var 0.000s 0.00kB/s 0.00kbps 14.1Ki 42.4Ki 128Ki 2048Mi 10.7ms/5341 1.120s 1638kB/s 13.1Mbps 132Ki 323Ki 790Ki 70.7Ki 106.2ms/574 2.094s 1481kB/s 11.8Mbps 189Ki 416Ki 1139Ki 70.7Ki 152.8ms/1331 3.018s 1418kB/s 11.3Mbps 243Ki 535Ki 1462Ki 70.7Ki 196.7ms/1030 4.226s 1411kB/s 11.3Mbps 314Ki 680Ki 1887Ki 70.7Ki 254.5ms/1929 5.196s 1351kB/s 10.8Mbps 370Ki 795Ki 2227Ki 70.7Ki 300.2ms/813 6.357s 1354kB/s 10.8Mbps 438Ki 955Ki 2635Ki 70.7Ki 355.6ms/856 7.009s 1408kB/s 11.3Mbps 475Ki 1021Ki 2856Ki 70.7Ki 386.2ms/374 8.444s 1461kB/s 11.7Mbps 560Ki 1213Ki 3366Ki 70.7Ki 455.5ms/1334 9.281s 1409kB/s 11.3Mbps 608Ki 1326Ki 3655Ki 70.7Ki 494.9ms/801 10.160s 1342kB/s 10.7Mbps 660Ki 1399Ki 3970Ki 70.7Ki 537.0ms/1078 Transferred 14.5MBytes in 12.180s, 111 syscalls, 131072.0 Bytes/syscall This is probably the best case, as all availabe bandwidth is utilized.","title":"Bandwidth limit"},{"location":"throughput/#performance","text":"According to Understanding Host Network Stack Overheads SIGCOMM'21, modern Linux network stack can achieve ~42Gbps throughput-per-core . In other words, a single TCP connection can sustain a 40Gbps NIC unidirectionally, but not an 100Gbps NIC. Eric reported 170Gbps single flow on 200Gbps NIC in netdev conf 2022-10, with receiver zero copy and BIG TCP .","title":"Performance"},{"location":"walkthrough/","text":"Code Walkthrough Blocking write packetdrill/gtests/net/tcp/blocking/blocking-write.pkt // Test for blocking write. --tolerance_usecs=10000 `../common/defaults.sh ../common/set_sysctls.py /proc/sys/net/ipv4/tcp_min_tso_segs=10 ` // Establish a connection. 0 socket(..., SOCK_STREAM,PROTO_TCP) = 3 +0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0 +0 bind(3, ..., ...) = 0 +0 listen(3, 1) = 0 +.1 < S 0:0(0) win 50000 <mss 1000,nop,wscale 0> +0 > S. 0:0(0) ack 1 <mss 1460,nop,wscale 8> +.1 < . 1:1(0) ack 1 win 50000 +0 accept(3, ..., ...) = 4 // Kernel doubles our value -> sk->sk_sndbuf is set to 42000 +0 setsockopt(4, SOL_SOCKET, SO_SNDBUF, [21000], 4) = 0 +0 getsockopt(4, SOL_SOCKET, SO_SNDBUF, [42000], [4]) = 0 // A write of 60000 does not block. +0...0.300 write(4, ..., 61000) = 61000 // this write() blocks +.1 < . 1:1(0) ack 10001 win 50000 +.1 < . 1:1(0) ack 30001 win 50000 // This ACK should wakeup the write(). An ACK of 35001 does not. +.1 < . 1:1(0) ack 36001 win 50000 // Reset to sysctls defaults. `/tmp/sysctl_restore_${PPID}.sh` tcpdump -i any -n -ttt tcp port 8080 // Three-way handshake 1 0.000000 remote:54321 > local:8080: [S], seq 0, win 50000, options [mss 1000,nop,wscale 0], length 0 2 0.000640 local:8080 > remote:54321: [S.], seq 12345, ack 1, win 65535, options [mss 1460,nop,wscale 8], length 0 3 0.111259 remote:54321 > local:8080: [.], ack 1, win 50000, length 0 // cwnd = 10, mss = 1000, so send 10 * 1000 then wait for ACK. 4 0.017588 local:8080 > remote:54321: [P.], seq 1:5001, ack 1, win 256, length 5000 5 0.000199 local:8080 > remote:54321: [P.], seq 5001:10001, ack 1, win 256, length 5000 6 0.101236 remote:54321 > local:8080: [.], ack 10001, win 50000, length 0 // slow-start, increase cwnd per ACK. cwnd = 20, so send 20 * 1000 then wait for ACK. 7 0.000573 local:8080 > remote:54321: [P.], seq 10001:20001, ack 1, win 256, length 10000 8 0.000276 local:8080 > remote:54321: [P.], seq 20001:30001, ack 1, win 256, length 10000 9 0.099876 remote:54321 > local:8080: [.], ack 30001, win 50000, length 0 // slow-start, again. write() now blocks. 10 0.000490 local:8080 > remote:54321: [P.], seq 30001:35001, ack 1, win 256, length 5000 11 0.000456 local:8080 > remote:54321: [P.], seq 35001:45001, ack 1, win 256, length 10000 12 0.000182 local:8080 > remote:54321: [P.], seq 45001:55001, ack 1, win 256, length 10000 13 0.000157 local:8080 > remote:54321: [P.], seq 55001:60001, ack 1, win 256, length 5000 14 0.098661 remote:54321 > local:8080: [.], ack 36001, win 50000, length 0 // the previous ACK unblocks write(). 15 0.001139 local:8080 > remote:54321: [P.], seq 60001:61001, ack 1, win 256, length 1000 16 0.325737 local:8080 > remote:54321: [.], seq 36001:37001, ack 1, win 256, length 1000 // Re-xmit 17 0.038498 local:8080 > remote:54321: [F.], seq 61001, ack 1, win 256, length 0 tcp_sendmsg(size=61k), cwnd=10 size_goal = 25k 1. copy 25k to skb sk_wmem_queued = 1280 + 25000 = 26280 tcp_push_one tcp_write_xmit skb1 = 25k, pfrag = 25000/32768 cwnd_quota = 5 tso_fragment splits skb to 5k + 20k, sk_wmem_queued += 1280, skb1 = 5k, skb2 = 20k tcp_transmit_skb(5k) sk->tcp_rtx_queue.insert(skb1) 2. copy 5k, copy = size_goal - tcp_write_queue_tail(sk)->len = 25k - 20k = 5k skb2 += 5k, pfrag = 30000/32768 sk_wmem_queued = 27560 + 5000 = 32560 __tcp_push_pending_frames tcp_write_xmit cwnd_quota = 5 tso_fragment splits skb to 5k + 20k, sk_wmem_queued += 1280, skb2 = 5k, skb3 = 20k tcp_transmit_skb(5k) sk->tcp_rtx_queue.insert(skb2) 3. copy 2768 (copied=32768) sk_wmem_queued = 33840 + 2768 = 36608 sk->sk_write_queue = [skb3(len=22768)] 4. copy 2232 (copied=35000), size_goal - skb->len = 2232 alloc a new page frag, WHY no new skb? sk_wmem_queued = 36608 + 2232 = 38840 tcp_push_one cwnd_quota=0 5. copy 25k alloc a new skb, sk_wmem_queued += 1280 sk_wmem_queued = 40120 + 25000 = 65120 tcp_push_one cwnd_quota=0 (copied=60k) 6. trying to copy remaining 1k sk->sk_wmem_queued (65120) > sk->sk_sndbuf (42000) wait_for_space 1st ack 10000, cwnd=20 tcp_rcv_established tcp_ack tcp_clean_rtx_queue tcp_rtx_queue_unlink_and_free sk_wmem_free_skb tcp_data_snd_check tcp_push_pending_frames tcp_write_xmit tcp_transmit_skb(10k) tcp_transmit_skb(10k) sk->sk_wmem_queued: 65120 -> 55120 tcp_check_space tcp_new_space sk_stream_write_space, stream_wspace=-13120 stream_min_wspace=27560) 2nd ack 30000, cwnd=40 tcp_write_xmit tcp_transmit_skb(5k) tcp_transmit_skb(10k) tcp_transmit_skb(10k) tcp_transmit_skb(5k) sk->sk_wmem_queued: 55120 -> 35120 sk_stream_write_space, stream_wspace=6880 stream_min_wspace=17560) 3rd ack 36000 sk->sk_wmem_queued: 35120 -> 27840 sk_stream_write_space, stream_wspace=14160 stream_min_wspace=13920 __sk_stream_is_writeable=true, wake up tcp_sendmsg wakes up tcp_transmit_skb(1k) Call trace of SYN, SYNACK __do_softirq -> net_rx_action -> napi_poll -> virtnet_poll -> virtqueue_napi_complete -> napi_complete_done -> gro_normal_list -> netif_receive_skb_list_internal -> __netif_receive_skb_list -> __netif_receive_skb_list_core -> __netif_receive_skb_list_ptype -> ip_list_rcv -> ip_sublist_rcv -> ip_list_rcv_finish -> ip_sublist_rcv_finish -> dst_input -> ip_local_deliver -> ip_local_deliver_finish -> IPv4 ip_local_deliver_finish -> ip_protocol_deliver_rcu -> tcp_v4_rcv -> tcp_v4_do_rcv -> tcp_rcv_state_process -> tcp_v4_conn_request -> tcp_conn_request -> tcp_v4_send_synack -> ip_output __do_softirq -> net_rx_action -> napi_poll -> process_backlog -> __netif_receive_skb -> __netif_receive_skb_one_core -> ip6_input -> IPv6 ip6_input -> ip6_input_finish -> ip6_protocol_deliver_rcu -> tcp_v6_rcv -> tcp_v6_do_rcv -> tcp_rcv_state_process -> tcp_v6_conn_request -> tcp_conn_request -> tcp_v6_send_synack -> ip6_xmit -> dst_output -> ip6_output Call trace of read(2) entry_SYSCALL_64 -> do_syscall_64 -> ksys_read -> vfs_read -> new_sync_read -> call_read_iter -> sock_read_iter -> sock_recvmsg -> sock_recvmsg_nosec -> inet_recvmsg -> tcp_recvmsg Call trace of readv(2) entry_SYSCALL_64 -> do_syscall_x64 -> __x64_sys_readv -> __do_sys_readv -> do_readv -> vfs_readv -> do_iter_read -> do_iter_readv_writev -> call_read_iter -> sock_read_iter -> sock_recvmsg -> sock_recvmsg_nosec -> inet_recvmsg -> tcp_recvmsg Call trace of recvfrom entry_SYSCALL_64 -> do_syscall_64 -> __x64_sys_recvfrom -> __se_sys_recvfrom -> __do_sys_recvfrom -> __sys_recvfrom -> sock_recvmsg Call trace of recvmsg entry_SYSCALL_64 -> do_syscall_64 -> __sys_recvmsg -> ___sys_recvmsg -> ____sys_recvmsg -> sock_recvmsg Call trace of sendto entry_SYSCALL_64 -> do_syscall_64 -> __x64_sys_sendto -> __se_sys_sendto -> __do_sys_sendto -> __sys_sendto -> sock_sendmsg Call trace of sendmsg entry_SYSCALL_64 -> do_syscall_64 -> __sys_sendmsg -> ___sys_sendmsg -> ____sys_sendmsg -> sock_sendmsg Call trace of write(2) entry_SYSCALL_64 -> do_syscall_64 -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter -> sock_write_iter -> sock_sendmsg -> sock_sendmsg_nosec -> tcp_sendmsg -> tcp_sendmsg_locked -> tcp_push -> __tcp_push_pending_frames -> tcp_write_xmit -> tcp_transmit_skb -> __tcp_transmit_skb -> ip_queue_xmit -> __ip_queue_xmit -> ip_local_out -> dst_output -> ip_output -> ip_finish_output -> __ip_finish_output -> ip_finish_output2 -> neigh_output IPv6 __tcp_transmit_skb -> inet6_csk_xmit -> ip6_xmit -> dst_output -> ip6_output -> ip6_finish_output2 -> neigh_output -> neigh_hh_output -> dev_queue_xmit -> __dev_queue_xmit -> __dev_xmit_skb -> qdisc_run -> __qdisc_run -> qdisc_restart -> sch_direct_xmit -> dev_hard_start_xmit -> xmit_one -> netdev_start_xmit -> __netdev_start_xmit -> mlx4_en_* Call trace of receiving ACK from packet 6 ip_local_deliver_finish -> ip_protocol_deliver_rcu -> tcp_v4_rcv -> tcp_v4_do_rcv -> tcp_rcv_established -> tcp_data_snd_check -> tcp_push_pending_frames -> __tcp_push_pending_frames -> ... Call trace of close(2) __fput -> sock_close -> __sock_release -> inet_release -> tcp_close -> tcp_send_fin -> __tcp_push_pending_frames -> tcp_write_xmit -> tcp_transmit_skb -> ...","title":"Walkthrough"},{"location":"walkthrough/#code-walkthrough","text":"","title":"Code Walkthrough"},{"location":"walkthrough/#blocking-write","text":"packetdrill/gtests/net/tcp/blocking/blocking-write.pkt // Test for blocking write. --tolerance_usecs=10000 `../common/defaults.sh ../common/set_sysctls.py /proc/sys/net/ipv4/tcp_min_tso_segs=10 ` // Establish a connection. 0 socket(..., SOCK_STREAM,PROTO_TCP) = 3 +0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0 +0 bind(3, ..., ...) = 0 +0 listen(3, 1) = 0 +.1 < S 0:0(0) win 50000 <mss 1000,nop,wscale 0> +0 > S. 0:0(0) ack 1 <mss 1460,nop,wscale 8> +.1 < . 1:1(0) ack 1 win 50000 +0 accept(3, ..., ...) = 4 // Kernel doubles our value -> sk->sk_sndbuf is set to 42000 +0 setsockopt(4, SOL_SOCKET, SO_SNDBUF, [21000], 4) = 0 +0 getsockopt(4, SOL_SOCKET, SO_SNDBUF, [42000], [4]) = 0 // A write of 60000 does not block. +0...0.300 write(4, ..., 61000) = 61000 // this write() blocks +.1 < . 1:1(0) ack 10001 win 50000 +.1 < . 1:1(0) ack 30001 win 50000 // This ACK should wakeup the write(). An ACK of 35001 does not. +.1 < . 1:1(0) ack 36001 win 50000 // Reset to sysctls defaults. `/tmp/sysctl_restore_${PPID}.sh` tcpdump -i any -n -ttt tcp port 8080 // Three-way handshake 1 0.000000 remote:54321 > local:8080: [S], seq 0, win 50000, options [mss 1000,nop,wscale 0], length 0 2 0.000640 local:8080 > remote:54321: [S.], seq 12345, ack 1, win 65535, options [mss 1460,nop,wscale 8], length 0 3 0.111259 remote:54321 > local:8080: [.], ack 1, win 50000, length 0 // cwnd = 10, mss = 1000, so send 10 * 1000 then wait for ACK. 4 0.017588 local:8080 > remote:54321: [P.], seq 1:5001, ack 1, win 256, length 5000 5 0.000199 local:8080 > remote:54321: [P.], seq 5001:10001, ack 1, win 256, length 5000 6 0.101236 remote:54321 > local:8080: [.], ack 10001, win 50000, length 0 // slow-start, increase cwnd per ACK. cwnd = 20, so send 20 * 1000 then wait for ACK. 7 0.000573 local:8080 > remote:54321: [P.], seq 10001:20001, ack 1, win 256, length 10000 8 0.000276 local:8080 > remote:54321: [P.], seq 20001:30001, ack 1, win 256, length 10000 9 0.099876 remote:54321 > local:8080: [.], ack 30001, win 50000, length 0 // slow-start, again. write() now blocks. 10 0.000490 local:8080 > remote:54321: [P.], seq 30001:35001, ack 1, win 256, length 5000 11 0.000456 local:8080 > remote:54321: [P.], seq 35001:45001, ack 1, win 256, length 10000 12 0.000182 local:8080 > remote:54321: [P.], seq 45001:55001, ack 1, win 256, length 10000 13 0.000157 local:8080 > remote:54321: [P.], seq 55001:60001, ack 1, win 256, length 5000 14 0.098661 remote:54321 > local:8080: [.], ack 36001, win 50000, length 0 // the previous ACK unblocks write(). 15 0.001139 local:8080 > remote:54321: [P.], seq 60001:61001, ack 1, win 256, length 1000 16 0.325737 local:8080 > remote:54321: [.], seq 36001:37001, ack 1, win 256, length 1000 // Re-xmit 17 0.038498 local:8080 > remote:54321: [F.], seq 61001, ack 1, win 256, length 0 tcp_sendmsg(size=61k), cwnd=10 size_goal = 25k 1. copy 25k to skb sk_wmem_queued = 1280 + 25000 = 26280 tcp_push_one tcp_write_xmit skb1 = 25k, pfrag = 25000/32768 cwnd_quota = 5 tso_fragment splits skb to 5k + 20k, sk_wmem_queued += 1280, skb1 = 5k, skb2 = 20k tcp_transmit_skb(5k) sk->tcp_rtx_queue.insert(skb1) 2. copy 5k, copy = size_goal - tcp_write_queue_tail(sk)->len = 25k - 20k = 5k skb2 += 5k, pfrag = 30000/32768 sk_wmem_queued = 27560 + 5000 = 32560 __tcp_push_pending_frames tcp_write_xmit cwnd_quota = 5 tso_fragment splits skb to 5k + 20k, sk_wmem_queued += 1280, skb2 = 5k, skb3 = 20k tcp_transmit_skb(5k) sk->tcp_rtx_queue.insert(skb2) 3. copy 2768 (copied=32768) sk_wmem_queued = 33840 + 2768 = 36608 sk->sk_write_queue = [skb3(len=22768)] 4. copy 2232 (copied=35000), size_goal - skb->len = 2232 alloc a new page frag, WHY no new skb? sk_wmem_queued = 36608 + 2232 = 38840 tcp_push_one cwnd_quota=0 5. copy 25k alloc a new skb, sk_wmem_queued += 1280 sk_wmem_queued = 40120 + 25000 = 65120 tcp_push_one cwnd_quota=0 (copied=60k) 6. trying to copy remaining 1k sk->sk_wmem_queued (65120) > sk->sk_sndbuf (42000) wait_for_space 1st ack 10000, cwnd=20 tcp_rcv_established tcp_ack tcp_clean_rtx_queue tcp_rtx_queue_unlink_and_free sk_wmem_free_skb tcp_data_snd_check tcp_push_pending_frames tcp_write_xmit tcp_transmit_skb(10k) tcp_transmit_skb(10k) sk->sk_wmem_queued: 65120 -> 55120 tcp_check_space tcp_new_space sk_stream_write_space, stream_wspace=-13120 stream_min_wspace=27560) 2nd ack 30000, cwnd=40 tcp_write_xmit tcp_transmit_skb(5k) tcp_transmit_skb(10k) tcp_transmit_skb(10k) tcp_transmit_skb(5k) sk->sk_wmem_queued: 55120 -> 35120 sk_stream_write_space, stream_wspace=6880 stream_min_wspace=17560) 3rd ack 36000 sk->sk_wmem_queued: 35120 -> 27840 sk_stream_write_space, stream_wspace=14160 stream_min_wspace=13920 __sk_stream_is_writeable=true, wake up tcp_sendmsg wakes up tcp_transmit_skb(1k) Call trace of SYN, SYNACK __do_softirq -> net_rx_action -> napi_poll -> virtnet_poll -> virtqueue_napi_complete -> napi_complete_done -> gro_normal_list -> netif_receive_skb_list_internal -> __netif_receive_skb_list -> __netif_receive_skb_list_core -> __netif_receive_skb_list_ptype -> ip_list_rcv -> ip_sublist_rcv -> ip_list_rcv_finish -> ip_sublist_rcv_finish -> dst_input -> ip_local_deliver -> ip_local_deliver_finish -> IPv4 ip_local_deliver_finish -> ip_protocol_deliver_rcu -> tcp_v4_rcv -> tcp_v4_do_rcv -> tcp_rcv_state_process -> tcp_v4_conn_request -> tcp_conn_request -> tcp_v4_send_synack -> ip_output __do_softirq -> net_rx_action -> napi_poll -> process_backlog -> __netif_receive_skb -> __netif_receive_skb_one_core -> ip6_input -> IPv6 ip6_input -> ip6_input_finish -> ip6_protocol_deliver_rcu -> tcp_v6_rcv -> tcp_v6_do_rcv -> tcp_rcv_state_process -> tcp_v6_conn_request -> tcp_conn_request -> tcp_v6_send_synack -> ip6_xmit -> dst_output -> ip6_output Call trace of read(2) entry_SYSCALL_64 -> do_syscall_64 -> ksys_read -> vfs_read -> new_sync_read -> call_read_iter -> sock_read_iter -> sock_recvmsg -> sock_recvmsg_nosec -> inet_recvmsg -> tcp_recvmsg Call trace of readv(2) entry_SYSCALL_64 -> do_syscall_x64 -> __x64_sys_readv -> __do_sys_readv -> do_readv -> vfs_readv -> do_iter_read -> do_iter_readv_writev -> call_read_iter -> sock_read_iter -> sock_recvmsg -> sock_recvmsg_nosec -> inet_recvmsg -> tcp_recvmsg Call trace of recvfrom entry_SYSCALL_64 -> do_syscall_64 -> __x64_sys_recvfrom -> __se_sys_recvfrom -> __do_sys_recvfrom -> __sys_recvfrom -> sock_recvmsg Call trace of recvmsg entry_SYSCALL_64 -> do_syscall_64 -> __sys_recvmsg -> ___sys_recvmsg -> ____sys_recvmsg -> sock_recvmsg Call trace of sendto entry_SYSCALL_64 -> do_syscall_64 -> __x64_sys_sendto -> __se_sys_sendto -> __do_sys_sendto -> __sys_sendto -> sock_sendmsg Call trace of sendmsg entry_SYSCALL_64 -> do_syscall_64 -> __sys_sendmsg -> ___sys_sendmsg -> ____sys_sendmsg -> sock_sendmsg Call trace of write(2) entry_SYSCALL_64 -> do_syscall_64 -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter -> sock_write_iter -> sock_sendmsg -> sock_sendmsg_nosec -> tcp_sendmsg -> tcp_sendmsg_locked -> tcp_push -> __tcp_push_pending_frames -> tcp_write_xmit -> tcp_transmit_skb -> __tcp_transmit_skb -> ip_queue_xmit -> __ip_queue_xmit -> ip_local_out -> dst_output -> ip_output -> ip_finish_output -> __ip_finish_output -> ip_finish_output2 -> neigh_output IPv6 __tcp_transmit_skb -> inet6_csk_xmit -> ip6_xmit -> dst_output -> ip6_output -> ip6_finish_output2 -> neigh_output -> neigh_hh_output -> dev_queue_xmit -> __dev_queue_xmit -> __dev_xmit_skb -> qdisc_run -> __qdisc_run -> qdisc_restart -> sch_direct_xmit -> dev_hard_start_xmit -> xmit_one -> netdev_start_xmit -> __netdev_start_xmit -> mlx4_en_* Call trace of receiving ACK from packet 6 ip_local_deliver_finish -> ip_protocol_deliver_rcu -> tcp_v4_rcv -> tcp_v4_do_rcv -> tcp_rcv_established -> tcp_data_snd_check -> tcp_push_pending_frames -> __tcp_push_pending_frames -> ... Call trace of close(2) __fput -> sock_close -> __sock_release -> inet_release -> tcp_close -> tcp_send_fin -> __tcp_push_pending_frames -> tcp_write_xmit -> tcp_transmit_skb -> ...","title":"Blocking write"}]}